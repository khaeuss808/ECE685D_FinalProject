{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall @K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import BertModel\n",
    "from torchvision import models\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: MPS (Apple)\n"
     ]
    }
   ],
   "source": [
    "# will check and use MPS if available, otherwise CUDA, otherwise CPU\n",
    "# mps is super fast mac thing\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using device: MPS (Apple)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using device: CUDA (GPU) - {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using device: CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_chckpt = torch.load(\"checkpoint_iter_14000.pt\", map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint keys: dict_keys(['epoch', 'iteration', 'text_enc', 'vision_enc', 'optimizer', 'scaler', 'train_loss'])\n"
     ]
    }
   ],
   "source": [
    "print(\"checkpoint keys:\", best_chckpt.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our encoder class definitions from training\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "MODEL_NAME = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "\n",
    "\n",
    "class ClinicalTextEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=512):\n",
    "        super(ClinicalTextEncoder, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(MODEL_NAME)\n",
    "        self.proj = nn.Linear(768, embed_dim)\n",
    "\n",
    "    def forward(self, token_ids, attention_masks):\n",
    "        outputs = self.bert(token_ids, attention_mask=attention_masks)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        embeddings = self.proj(cls_embeddings)\n",
    "        # normalizing because we need to compare with image embeddings later\n",
    "        # for the contrastive similarity\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=-1, eps=1e-6)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "# Use the torchvision's implementation of ResNeXt, but add FC layer to generate 512d embedding.\n",
    "class VisionEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=512):\n",
    "        super().__init__()\n",
    "        resnet = models.resnext50_32x4d(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        in_dim = resnet.fc.in_features\n",
    "        self.proj = nn.Linear(in_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        features = features.squeeze(-1).squeeze(-1)\n",
    "        z = self.proj(features)\n",
    "        # convert to unit vectors for cosine similarity later\n",
    "        z = z / z.norm(dim=-1, keepdim=True)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87959486adaf4a5a8be00e9fef7a8631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0458815069664b0dbf528b2aea27e8cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt50_32X4D_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "text_encoder = ClinicalTextEncoder(embed_dim=512)\n",
    "image_encoder = VisionEncoder(embed_dim=512)\n",
    "\n",
    "# load the trained weights\n",
    "text_encoder.load_state_dict(best_chckpt[\"text_enc\"])\n",
    "image_encoder.load_state_dict(best_chckpt[\"vision_enc\"])\n",
    "\n",
    "text_enc = text_encoder.to(device).eval()\n",
    "image_encoder = image_encoder.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report_updated(row):\n",
    "    labels = row.iloc[5:]  # Skip Path, Sex, Age, Frontal/Lateral, AP/PA\n",
    "\n",
    "    # Separate findings by certainty\n",
    "    positive_findings = list(labels[labels == 1.0].index)\n",
    "    uncertain_findings = list(labels[labels == -1.0].index)\n",
    "\n",
    "    # Build the report\n",
    "    report_parts = []\n",
    "\n",
    "    # Add patient demographics for context\n",
    "    age = int(row[\"Age\"]) if pd.notna(row[\"Age\"]) else None\n",
    "    sex = row[\"Sex\"].lower() if pd.notna(row[\"Sex\"]) else None\n",
    "    view = row[\"Frontal/Lateral\"].lower() if pd.notna(row[\"Frontal/Lateral\"]) else None\n",
    "\n",
    "    # Start with view type\n",
    "    if view:\n",
    "        report_parts.append(f\"{view.capitalize()} chest radiograph\")\n",
    "    else:\n",
    "        report_parts.append(\"Chest radiograph\")\n",
    "\n",
    "    # Add demographics\n",
    "    demo = []\n",
    "    if age:\n",
    "        demo.append(f\"{age}-year-old\")\n",
    "    if sex:\n",
    "        demo.append(sex)\n",
    "    if demo:\n",
    "        report_parts.append(f\"of {' '.join(demo)} patient\")\n",
    "\n",
    "    # Add findings\n",
    "    if len(positive_findings) == 0 and len(uncertain_findings) == 0:\n",
    "        report_parts.append(\"demonstrates no acute cardiopulmonary abnormality\")\n",
    "    else:\n",
    "        findings_text = []\n",
    "\n",
    "        # Definite findings\n",
    "        if positive_findings:\n",
    "            findings_clean = [f.lower().replace(\"_\", \" \") for f in positive_findings]\n",
    "            findings_text.append(\"shows \" + \", \".join(findings_clean))\n",
    "\n",
    "        # Uncertain findings (optional - you might want to treat these differently)\n",
    "        if uncertain_findings:\n",
    "            uncertain_clean = [f.lower().replace(\"_\", \" \") for f in uncertain_findings]\n",
    "            findings_text.append(\"possible \" + \", \".join(uncertain_clean))\n",
    "\n",
    "        report_parts.append(\". \".join(findings_text))\n",
    "\n",
    "    return \" \".join(report_parts) + \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset definition from training file\n",
    "class ClinicalCustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        df = df.reset_index(drop=True)  # Reset index to ensure 0-based indexing\n",
    "        # Text\n",
    "        self.text_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        self.reports = df.apply(\n",
    "            generate_report_updated, axis=1\n",
    "        )  # Maybe move generate report to inside the dataset later\n",
    "\n",
    "        # Vision\n",
    "        self.images = df[\"Path\"]\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reports)  # This could work or we could do another way\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Text part\n",
    "        report = self.reports[idx]\n",
    "        encoder = self.text_tokenizer.encode_plus(\n",
    "            report,\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        # Vision part\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        img_tensor = self.transform(image)\n",
    "\n",
    "        return {\n",
    "            \"token_ids\": encoder[\"input_ids\"],\n",
    "            \"attention_masks\": encoder[\"attention_mask\"],\n",
    "            \"image_tensor\": img_tensor,\n",
    "            \"report\": report,\n",
    "            \"img_path\": img_path,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate embeddings for validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied these blocks from the training notebook for getting val loader\n",
    "val_full = pd.read_csv(\"CheXpert-v1.0-small/valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "val_dataset = ClinicalCustomDataset(val_full)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all image and text embeddings\n",
    "image_embeddings = []\n",
    "text_embeddings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        images = batch[\"image_tensor\"].to(device)\n",
    "        token_ids = batch[\"token_ids\"].squeeze(1).to(device)\n",
    "        attention_masks = batch[\"attention_masks\"].squeeze(1).to(device)\n",
    "\n",
    "        img_emb = image_encoder(images)\n",
    "        txt_emb = text_encoder(token_ids, attention_masks)\n",
    "\n",
    "        image_embeddings.append(img_emb.cpu())\n",
    "        text_embeddings.append(txt_emb.cpu())\n",
    "\n",
    "# Concatenate all batches\n",
    "image_embeddings = torch.cat(image_embeddings, dim=0)\n",
    "text_embeddings = torch.cat(text_embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement recall @k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(image_embeds, text_embeds, k_values=[1, 5, 10]):\n",
    "    \"\"\"\n",
    "    Compute Recall@K for image-to-text and text-to-image retrieval\n",
    "    \"\"\"\n",
    "    # Compute similarity matrix: [N_images, N_texts]\n",
    "    similarity = torch.matmul(image_embeds, text_embeds.T)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Image-to-Text Recall@K\n",
    "    for k in k_values:\n",
    "        # For each image, get top-k most similar texts\n",
    "        top_k_indices = similarity.topk(k, dim=1).indices  # [N, k]\n",
    "\n",
    "        # Check if correct text (same index) is in top-k\n",
    "        correct = torch.zeros(len(image_embeds))\n",
    "        for i in range(len(image_embeds)):\n",
    "            if i in top_k_indices[i]:\n",
    "                correct[i] = 1\n",
    "\n",
    "        recall = correct.mean().item()\n",
    "        results[f\"image_to_text_recall@{k}\"] = recall\n",
    "\n",
    "    # Text-to-Image Recall@K\n",
    "    for k in k_values:\n",
    "        # For each text, get top-k most similar images\n",
    "        top_k_indices = similarity.T.topk(k, dim=1).indices  # [N, k]\n",
    "\n",
    "        # Check if correct image (same index) is in top-k\n",
    "        correct = torch.zeros(len(text_embeds))\n",
    "        for i in range(len(text_embeds)):\n",
    "            if i in top_k_indices[i]:\n",
    "                correct[i] = 1\n",
    "\n",
    "        recall = correct.mean().item()\n",
    "        results[f\"text_to_image_recall@{k}\"] = recall\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_to_text_recall@1: 0.2521\n",
      "image_to_text_recall@5: 0.6453\n",
      "image_to_text_recall@10: 0.8162\n",
      "text_to_image_recall@1: 0.2650\n",
      "text_to_image_recall@5: 0.6282\n",
      "text_to_image_recall@10: 0.7735\n"
     ]
    }
   ],
   "source": [
    "# Compute metrics\n",
    "k_values = [1, 5, 10]\n",
    "results = recall_at_k(image_embeddings, text_embeddings, k_values)\n",
    "\n",
    "# Print results\n",
    "for metric, value in results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
