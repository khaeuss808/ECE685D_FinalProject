{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall @K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from text_encoding_exp import TextEncoder, VisionEncoder\n",
    "\n",
    "# import from the training notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will check and use MPS if available, otherwise CUDA, otherwise CPU\n",
    "# mps is super fast mac thing\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using device: MPS (Apple)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using device: CUDA (GPU) - {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using device: CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained weights\n",
    "text_enc = TextEncoder(embed_dim=512)\n",
    "vision_enc = VisionEncoder(embed_dim=512)\n",
    "\n",
    "text_enc.load_state_dict(torch.load(\"text_encoder.pth\"))\n",
    "vision_enc.load_state_dict(torch.load(\"vision_encoder.pth\"))\n",
    "\n",
    "text_enc.eval()\n",
    "vision_enc.eval()\n",
    "text_enc = text_enc.to(device)\n",
    "vision_enc = vision_enc.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate embeddings for validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied these blocks from the training notebook for getting val loader\n",
    "val_full = pd.read_csv(\"CheXpert-v1.0-small/valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = CustomDataset(val_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all image and text embeddings\n",
    "image_embeddings = []\n",
    "text_embeddings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        images = batch[\"image_tensor\"].to(device)\n",
    "        token_ids = batch[\"token_ids\"].squeeze(1).to(device)\n",
    "        attention_masks = batch[\"attention_masks\"].squeeze(1).to(device)\n",
    "\n",
    "        img_emb = vision_enc(images)\n",
    "        txt_emb = text_enc(token_ids, attention_masks)\n",
    "\n",
    "        image_embeddings.append(img_emb.cpu())\n",
    "        text_embeddings.append(txt_emb.cpu())\n",
    "\n",
    "# Concatenate all batches\n",
    "image_embeddings = torch.cat(image_embeddings, dim=0)  # Shape: [N, 512]\n",
    "text_embeddings = torch.cat(text_embeddings, dim=0)  # Shape: [N, 512]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement recall @k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(image_embeds, text_embeds, k_values=[1, 5, 10]):\n",
    "    \"\"\"\n",
    "    Compute Recall@K for image-to-text and text-to-image retrieval\n",
    "    \"\"\"\n",
    "    # Compute similarity matrix: [N_images, N_texts]\n",
    "    similarity = torch.matmul(image_embeds, text_embeds.T)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Image-to-Text Recall@K\n",
    "    for k in k_values:\n",
    "        # For each image, get top-k most similar texts\n",
    "        top_k_indices = similarity.topk(k, dim=1).indices  # [N, k]\n",
    "\n",
    "        # Check if correct text (same index) is in top-k\n",
    "        correct = torch.zeros(len(image_embeds))\n",
    "        for i in range(len(image_embeds)):\n",
    "            if i in top_k_indices[i]:\n",
    "                correct[i] = 1\n",
    "\n",
    "        recall = correct.mean().item()\n",
    "        results[f\"image_to_text_recall@{k}\"] = recall\n",
    "\n",
    "    # Text-to-Image Recall@K\n",
    "    for k in k_values:\n",
    "        # For each text, get top-k most similar images\n",
    "        top_k_indices = similarity.T.topk(k, dim=1).indices  # [N, k]\n",
    "\n",
    "        # Check if correct image (same index) is in top-k\n",
    "        correct = torch.zeros(len(text_embeds))\n",
    "        for i in range(len(text_embeds)):\n",
    "            if i in top_k_indices[i]:\n",
    "                correct[i] = 1\n",
    "\n",
    "        recall = correct.mean().item()\n",
    "        results[f\"text_to_image_recall@{k}\"] = recall\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Compute metrics\n",
    "k_values = [1, 5, 10]\n",
    "results = recall_at_k(image_embeddings, text_embeddings, k_values)\n",
    "\n",
    "# Print results\n",
    "for metric, value in results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
