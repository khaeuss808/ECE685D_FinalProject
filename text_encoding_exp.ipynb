{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ac492df",
   "metadata": {},
   "source": [
    "Initial tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ec5ee36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/kaylahaeusssler/miniforge3/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/kaylahaeusssler/miniforge3/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/kaylahaeusssler/miniforge3/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/bs/tlrmg2n164520pry59rs7fvh0000gn/T/ipykernel_29605/932396916.py\", line 5, in <module>\n",
      "    from transformers import BertTokenizer, BertModel\n",
      "  File \"/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/transformers/__init__.py\", line 26, in <module>\n",
      "    from . import dependency_versions_check\n",
      "  File \"/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
      "    from .utils.versions import require_version, require_version_core\n",
      "  File \"/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/transformers/utils/__init__.py\", line 31, in <module>\n",
      "    from .generic import (\n",
      "  File \"/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/transformers/utils/generic.py\", line 432, in <module>\n",
      "    import torch.utils._pytree as _torch_pytree\n",
      "  File \"/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/torch/__init__.py\", line 1471, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! BertModel imported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/kaylahaeusssler/miniforge3/lib/python3.12/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Force CPU fallback for problematic MPS operations\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn as nn\n",
    "\n",
    "print(\"Success! BertModel imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59957ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "025500bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:07:06) [Clang 17.0.6 ]\n",
      "PyTorch: 2.4.0\n",
      "Transformers: 4.46.3\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bb42a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig imported!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Try importing with lazy loading disabled\n",
    "import os\n",
    "\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"0\"\n",
    "\n",
    "# Import just the model config first\n",
    "from transformers import BertConfig\n",
    "\n",
    "print(\"BertConfig imported!\")\n",
    "\n",
    "# Then try the model\n",
    "from transformers.models.bert.modeling_bert import BertModel\n",
    "\n",
    "print(\"BertModel imported via direct path!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "910a9ff6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Try importing BertModel by itself\n",
    "from transformers import BertModel\n",
    "\n",
    "print(\"BertModel class imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "912b50c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f853288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will check and use MPS if available, otherwise CUDA, otherwise CPU\n",
    "# mps is super fast mac thing\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using device: MPS (Apple)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using device: CUDA (GPU) - {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using device: CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e52f1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = pd.read_csv(\"CheXpert-v1.0-small/train.csv\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff99391f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Sex",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Age",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Frontal/Lateral",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "AP/PA",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "No Finding",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Enlarged Cardiomediastinum",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Cardiomegaly",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Lung Opacity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Lung Lesion",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Edema",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Consolidation",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Pneumonia",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Atelectasis",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Pneumothorax",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Pleural Effusion",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Pleural Other",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Fracture",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Support Devices",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "8b7a0f1e-d49a-4c46-8b51-2c36db66701c",
       "rows": [
        [
         "0",
         "CheXpert-v1.0-small/train/patient00001/study1/view1_frontal.jpg",
         "Female",
         "68",
         "Frontal",
         "AP",
         "1.0",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         null,
         null,
         null,
         "1.0"
        ],
        [
         "1",
         "CheXpert-v1.0-small/train/patient00002/study2/view1_frontal.jpg",
         "Female",
         "87",
         "Frontal",
         "AP",
         null,
         null,
         "-1.0",
         "1.0",
         null,
         "-1.0",
         "-1.0",
         null,
         "-1.0",
         null,
         "-1.0",
         null,
         "1.0",
         null
        ],
        [
         "2",
         "CheXpert-v1.0-small/train/patient00002/study1/view1_frontal.jpg",
         "Female",
         "83",
         "Frontal",
         "AP",
         null,
         null,
         null,
         "1.0",
         null,
         null,
         "-1.0",
         null,
         null,
         null,
         null,
         null,
         "1.0",
         null
        ],
        [
         "3",
         "CheXpert-v1.0-small/train/patient00002/study1/view2_lateral.jpg",
         "Female",
         "83",
         "Lateral",
         null,
         null,
         null,
         null,
         "1.0",
         null,
         null,
         "-1.0",
         null,
         null,
         null,
         null,
         null,
         "1.0",
         null
        ],
        [
         "4",
         "CheXpert-v1.0-small/train/patient00003/study1/view1_frontal.jpg",
         "Male",
         "41",
         "Frontal",
         "AP",
         null,
         null,
         null,
         null,
         null,
         "1.0",
         null,
         null,
         null,
         "0.0",
         null,
         null,
         null,
         null
        ]
       ],
       "shape": {
        "columns": 19,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Frontal/Lateral</th>\n",
       "      <th>AP/PA</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Support Devices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient00001/study1/...</td>\n",
       "      <td>Female</td>\n",
       "      <td>68</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>AP</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient00002/study2/...</td>\n",
       "      <td>Female</td>\n",
       "      <td>87</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>AP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient00002/study1/...</td>\n",
       "      <td>Female</td>\n",
       "      <td>83</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>AP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient00002/study1/...</td>\n",
       "      <td>Female</td>\n",
       "      <td>83</td>\n",
       "      <td>Lateral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient00003/study1/...</td>\n",
       "      <td>Male</td>\n",
       "      <td>41</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>AP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Path     Sex  Age  \\\n",
       "0  CheXpert-v1.0-small/train/patient00001/study1/...  Female   68   \n",
       "1  CheXpert-v1.0-small/train/patient00002/study2/...  Female   87   \n",
       "2  CheXpert-v1.0-small/train/patient00002/study1/...  Female   83   \n",
       "3  CheXpert-v1.0-small/train/patient00002/study1/...  Female   83   \n",
       "4  CheXpert-v1.0-small/train/patient00003/study1/...    Male   41   \n",
       "\n",
       "  Frontal/Lateral AP/PA  No Finding  Enlarged Cardiomediastinum  Cardiomegaly  \\\n",
       "0         Frontal    AP         1.0                         NaN           NaN   \n",
       "1         Frontal    AP         NaN                         NaN          -1.0   \n",
       "2         Frontal    AP         NaN                         NaN           NaN   \n",
       "3         Lateral   NaN         NaN                         NaN           NaN   \n",
       "4         Frontal    AP         NaN                         NaN           NaN   \n",
       "\n",
       "   Lung Opacity  Lung Lesion  Edema  Consolidation  Pneumonia  Atelectasis  \\\n",
       "0           NaN          NaN    NaN            NaN        NaN          NaN   \n",
       "1           1.0          NaN   -1.0           -1.0        NaN         -1.0   \n",
       "2           1.0          NaN    NaN           -1.0        NaN          NaN   \n",
       "3           1.0          NaN    NaN           -1.0        NaN          NaN   \n",
       "4           NaN          NaN    1.0            NaN        NaN          NaN   \n",
       "\n",
       "   Pneumothorax  Pleural Effusion  Pleural Other  Fracture  Support Devices  \n",
       "0           0.0               NaN            NaN       NaN              1.0  \n",
       "1           NaN              -1.0            NaN       1.0              NaN  \n",
       "2           NaN               NaN            NaN       1.0              NaN  \n",
       "3           NaN               NaN            NaN       1.0              NaN  \n",
       "4           0.0               NaN            NaN       NaN              NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b8bf01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Path', 'Sex', 'Age', 'Frontal/Lateral', 'AP/PA', 'No Finding',\n",
       "       'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
       "       'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
       "       'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture',\n",
       "       'Support Devices'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_subset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd42007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AP/PA is a view position column, whether Xray facing front or back\n",
    "# ignoring for now, but maybe we want to keep in the future??\n",
    "train_subset = train_subset.drop(\"AP/PA\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61531583",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = train_subset.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db26ed9",
   "metadata": {},
   "source": [
    "### Generate text reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1b54b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report(row):\n",
    "    labels = row.iloc[4:]\n",
    "    findings_list = list(labels[labels == 1].index)\n",
    "    findings = \", \".join(findings_list)\n",
    "    return f\"X-Ray report findings: {findings}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c0f1f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = train_subset.apply(generate_report, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4292d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82c4fee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence = test[0]\n",
    "\n",
    "token_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# Encode each review\n",
    "for report in test:\n",
    "    batch_encoder = tokenizer.encode_plus(\n",
    "        report,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    token_ids.append(batch_encoder[\"input_ids\"])\n",
    "    attention_masks.append(batch_encoder[\"attention_mask\"])\n",
    "\n",
    "# Convert token IDs and attention mask lists to PyTorch tensors\n",
    "token_ids = torch.cat(token_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10e130f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    output_hidden_states=True,  # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb8dfe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(token_ids, attention_mask=attention_masks)\n",
    "    word_embeddings = outputs.last_hidden_state  # This contains the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e59bef70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only embedding for CLS token\n",
    "CLS = word_embeddings[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70f14cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "final = nn.Linear(768, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ae8309a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 512])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final(CLS).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2775af5",
   "metadata": {},
   "source": [
    "Encoder Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a38716",
   "metadata": {},
   "source": [
    "### Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b06a3783",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=512):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\n",
    "            \"bert-base-uncased\",\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "        self.linear = nn.Linear(768, embed_dim)\n",
    "\n",
    "    def forward(self, token_ids, attention_masks):\n",
    "        outputs = self.bert(token_ids, attention_mask=attention_masks)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        embeddings = self.linear(cls_embeddings)\n",
    "        # normalizing because we need to compare with image embeddings later\n",
    "        # for the contrastive similarity\n",
    "        embeddings = embeddings / embeddings.norm(dim=-1, keepdim=True)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bc8a2c",
   "metadata": {},
   "source": [
    "### Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f97324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the torchvision's implementation of ResNeXt, but add FC layer to generate 512d embedding.\n",
    "class VisionEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=512):\n",
    "        super().__init__()\n",
    "        resnet = models.resnext50_32x4d(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        in_dim = resnet.fc.in_features\n",
    "        self.proj = nn.Linear(in_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        features = features.squeeze(-1).squeeze(-1)\n",
    "        z = self.proj(features)\n",
    "        # convert to unit vectors for cosine similarity later\n",
    "        z = z / z.norm(dim=-1, keepdim=True)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c4d62f",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b450ac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        # Text stuff\n",
    "        self.text_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.reports = df.apply(\n",
    "            generate_report, axis=1\n",
    "        )  # Maybe move generate report to inside the dataset idk\n",
    "\n",
    "        # Vision stuff?\n",
    "        self.images = df[\"Path\"]\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reports)  # This could work or we could do another way\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Text part\n",
    "        report = self.reports[idx]\n",
    "        encoder = self.text_tokenizer.encode_plus(\n",
    "            report,\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        # Vision part\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        img_tensor = self.transform(image)\n",
    "\n",
    "        return {\n",
    "            \"token_ids\": encoder[\"input_ids\"],\n",
    "            \"attention_masks\": encoder[\"attention_mask\"],\n",
    "            \"image_tensor\": img_tensor,\n",
    "        }  # Is vision part correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b6818e",
   "metadata": {},
   "source": [
    "Samples come in as dataframe and we put in dataset. For each sample, we can then return inputs for text encoder (token ids & attention masks) & for vision encoder (tensor of pixels). We pass relevant inputs to text encoder and vision encoder, get embeddings for that sample. Then blah blah"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3bf8db",
   "metadata": {},
   "source": [
    "Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "201af40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report(row):\n",
    "    labels = row.iloc[4:]\n",
    "    findings_list = list(labels[labels == 1].index)\n",
    "    findings = \", \".join(findings_list)\n",
    "    return f\"X-Ray report findings: {findings}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8305718",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = pd.read_csv(\"CheXpert-v1.0-small/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd2fdcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_full = pd.read_csv(\"CheXpert-v1.0-small/valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6213f6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishikarandev/miniforge3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/rishikarandev/miniforge3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt50_32X4D_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Initialize text encoder & vision encoder models\n",
    "text_enc = TextEncoder(embed_dim=512)\n",
    "\n",
    "# Vision\n",
    "vision_enc = VisionEncoder(embed_dim=512)\n",
    "\n",
    "\n",
    "# Create train & val datasets\n",
    "train_dataset = CustomDataset(train_full)\n",
    "val_dataset = CustomDataset(val_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5213b108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate text embedding\n",
    "text_enc(\n",
    "    token_ids=train_dataset[0][\"token_ids\"],\n",
    "    attention_masks=train_dataset[0][\"attention_masks\"],\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a952d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate img embedding\n",
    "vision_enc(train_dataset[0][\"image_tensor\"].unsqueeze(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "377f05ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf933c7",
   "metadata": {},
   "source": [
    "### Contrastive Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fba9bd",
   "metadata": {},
   "source": [
    "# image_encoder - ResNet or Vision Transformer\n",
    "# text_encoder - CBOW or Text Transformer\n",
    "# I[n, h, w, c] - minibatch of aligned images\n",
    "K: n is batch size, h,w is height width, c is number of channels (3 for RGB)\n",
    "\n",
    "# T[n, l] - minibatch of aligned texts\n",
    "K: n is batch size, l is sequence length (number of tokens)\n",
    "\n",
    "K: aligned means that they are paired up\n",
    "\n",
    "# W_i[d_i, d_e] - learned proj of image to embed\n",
    "# W_t[d_t, d_e] - learned proj of text to embed\n",
    "# t - learned temperature parameter\n",
    "\n",
    "# extract feature representations of each modality\n",
    "I_f = image_encoder(I) #[n, d_i] \n",
    "this happens in vision_enc\n",
    "K: get features from resnet\n",
    "\n",
    "T_f = text_encoder(T) #[n, d_t]\n",
    "this happens in text_enc\n",
    "K: get features from bert\n",
    "\n",
    "# joint multimodal embedding [n, d_e]\n",
    "K: this is projecting to joint embedding space\n",
    "I_e = l2_normalize(np.dot(I_f, W_i), axis=1)\n",
    "T_e = l2_normalize(np.dot(T_f, W_t), axis=1)\n",
    "\n",
    "# scaled pairwise cosine similarities [n, n]\n",
    "logits = np.dot(I_e, T_e.T) * np.exp(t) #K: scale by temperature\n",
    "\n",
    "\n",
    "# symmetric loss function\n",
    "labels = np.arange(n) #K: correct match is on the diagonal\n",
    "# K: now we compute symmetric loss\n",
    "loss_i = cross_entropy_loss(logits, labels, axis=0) #K: image to text\n",
    "loss_t = cross_entropy_loss(logits, labels, axis=1) #K: text to image\n",
    "# K: now average in both directions\n",
    "loss = (loss_i + loss_t)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f8dc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a lot of the steps from the pseudo code are already done in other places!! which is lit\n",
    "# I_f, T_f, I_e, T_e are all done in other places\n",
    "def contrastive_loss(image_embeddings, text_embeddings, temperature=0.07):\n",
    "    # we can make the t a fixed parameter but most CLIP reproductions make it fixed\n",
    "    # so just keeping it as argument for now\n",
    "\n",
    "    batch_size = image_embeddings.shape[0]  # called n in the pseudo code\n",
    "\n",
    "    # we already have normalized embeddings, so we can skip to cosine similarity part of pseudo code\n",
    "    # \"scaled pairwise cosine similarities [n, n]\"\n",
    "    # dividing by temperature to scale the logits\n",
    "    logits = torch.matmul(image_embeddings, text_embeddings.T) / temperature\n",
    "    # now \"symmetric loss function\"\n",
    "    labels = torch.arange(batch_size, device=image_embeddings.device)\n",
    "    loss_img_to_text = nn.CrossEntropyLoss()(logits, labels)\n",
    "    loss_text_to_img = nn.CrossEntropyLoss()(logits.T, labels)\n",
    "\n",
    "    # average loss in both directions\n",
    "    loss = (loss_img_to_text + loss_text_to_img) / 2\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f93aec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training setup, referencing homework code\n",
    "lr = 1e-3  # maybe we will adjust this\n",
    "batch_size = 32  # already set in the DataLoader\n",
    "temperature = 0.07\n",
    "max_epoch_number = 20\n",
    "test_freq = 50\n",
    "\n",
    "# move models to device\n",
    "text_enc = text_enc.to(device)\n",
    "vision_enc = vision_enc.to(device)\n",
    "\n",
    "# combine parameters of both models for the optimizer\n",
    "# our homework only used one model, so now we gotta grab both model params\n",
    "all_parameters = list(text_enc.parameters()) + list(vision_enc.parameters())\n",
    "optimizer = torch.optim.Adam(all_parameters, lr=lr)\n",
    "\n",
    "\n",
    "epoch = 0\n",
    "iteration = 0\n",
    "while True:\n",
    "    text_enc.train()\n",
    "    vision_enc.train()\n",
    "\n",
    "    batch_losses = []\n",
    "\n",
    "    for batch in train_loader:\n",
    "        images = batch[\"image_tensor\"].to(device)\n",
    "        token_ids = batch[\"token_ids\"].squeeze(1).to(device)  # Remove extra dimension\n",
    "        attention_masks = batch[\"attention_masks\"].squeeze(1).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # this is forward pass\n",
    "        image_embeddings = vision_enc(images)\n",
    "        text_embeddings = text_enc(token_ids, attention_masks)\n",
    "\n",
    "        # get loss\n",
    "        loss = contrastive_loss(image_embeddings, text_embeddings, temperature)\n",
    "\n",
    "        # now backward pass\n",
    "        batch_loss_value = loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(batch_loss_value)\n",
    "\n",
    "        # i had Claude rewrite this validation loss part\n",
    "        if iteration % test_freq == 0:\n",
    "            text_enc.eval()\n",
    "            vision_enc.eval()\n",
    "\n",
    "            val_losses = []  # Track validation losses\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch_val in val_loader:\n",
    "                    # Get validation data\n",
    "                    val_images = batch_val[\"image_tensor\"].to(device)\n",
    "                    val_token_ids = batch_val[\"token_ids\"].squeeze(1).to(device)\n",
    "                    val_attention_masks = (\n",
    "                        batch_val[\"attention_masks\"].squeeze(1).to(device)\n",
    "                    )\n",
    "\n",
    "                    # Forward pass\n",
    "                    val_img_embeds = vision_enc(val_images)\n",
    "                    val_txt_embeds = text_enc(val_token_ids, val_attention_masks)\n",
    "\n",
    "                    # Compute loss\n",
    "                    val_loss = contrastive_loss(\n",
    "                        val_img_embeds, val_txt_embeds, temperature\n",
    "                    )\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "            # Print validation results\n",
    "            avg_val_loss = np.mean(val_losses)\n",
    "            print(f\"epoch:{epoch:2d} iter:{iteration:3d} val: loss:{avg_val_loss:.3f}\")\n",
    "\n",
    "            # Back to training mode\n",
    "            text_enc.train()\n",
    "            vision_enc.train()\n",
    "        iteration += 1\n",
    "\n",
    "    train_loss = np.mean(batch_losses)\n",
    "    print(f\"epoch:{epoch:2d} iter:{iteration:4d} train loss:{train_loss:.3f}\\n\")\n",
    "\n",
    "    epoch += 1\n",
    "    if max_epoch_number < epoch:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64b52ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore for now just copied from our hw\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "batch_size = 32\n",
    "criterion = nn.BCELoss()\n",
    "max_epoch_number = 20\n",
    "test_freq = 50\n",
    "\n",
    "model = model.to(device)\n",
    "epoch = 0\n",
    "iteration = 0\n",
    "while True:\n",
    "    batch_losses = []\n",
    "    for imgs, targets in train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        model_result = model(imgs)\n",
    "        loss = criterion(model_result, targets.type(torch.float))\n",
    "\n",
    "        batch_loss_value = loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(batch_loss_value)\n",
    "\n",
    "        if iteration % test_freq == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                model_result = []\n",
    "                targets = []\n",
    "                for imgs, batch_targets in val_loader:\n",
    "                    imgs = imgs.to(device)\n",
    "                    batch_targets = batch_targets.to(device)\n",
    "                    model_batch_result = model(imgs)\n",
    "                    # model_pred_binary = (torch.sigmoid(model_batch_result) > 0.5).int().cpu().numpy()\n",
    "                    model_result.extend(model_batch_result.cpu().numpy())\n",
    "                    targets.extend(batch_targets.cpu().numpy())\n",
    "\n",
    "            result = calculate_metrics(np.array(model_result), np.array(targets))\n",
    "            print(\n",
    "                \"epoch:{:2d} iter:{:3d} test: \"\n",
    "                \"micro f1: {:.3f} \"\n",
    "                \"macro f1: {:.3f} \"\n",
    "                \"samples f1: {:.3f}\".format(\n",
    "                    epoch,\n",
    "                    iteration,\n",
    "                    result[\"micro/f1\"],\n",
    "                    result[\"macro/f1\"],\n",
    "                    result[\"samples/f1\"],\n",
    "                )\n",
    "            )\n",
    "\n",
    "            model.train()\n",
    "        iteration += 1\n",
    "\n",
    "    loss_value = np.mean(batch_losses)\n",
    "    print(\n",
    "        \"epoch:{:2d} iter:{:3d} train: loss:{:.3f}\".format(epoch, iteration, loss_value)\n",
    "    )\n",
    "    # if epoch % save_freq == 0:\n",
    "    #    checkpoint_save(model, save_path, epoch)\n",
    "    epoch += 1\n",
    "    if max_epoch_number < epoch:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f991b0f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
