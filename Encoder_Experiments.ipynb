{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9ac492df",
      "metadata": {
        "id": "9ac492df"
      },
      "source": [
        "Initial tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "59957ff0",
      "metadata": {
        "id": "59957ff0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import transformers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3f853288",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f853288",
        "outputId": "122f4f59-33dd-4ddc-d87a-39fa21c3a2a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: CUDA (GPU) - Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# will check and use CUDA if available, otherwise MPS, otherwise CPU\n",
        "# mps is super fast mac thing\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"Using device: MPS (Apple)\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"Using device: CUDA (GPU) - {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using device: CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies as needed:\n",
        "# pip install kagglehub[pandas-datasets]\n",
        "# For downloading entire datasets, install the kaggle API client:\n",
        "!pip install kaggle\n",
        "\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "# --------- Run once ---------\n",
        "# 1. Go to your Kaggle profile: https://www.kaggle.com/<your-username>/account\n",
        "# 2. Under 'API', click 'Create New API Token' to download `kaggle.json`.\n",
        "# 3. Upload `kaggle.json` to your Colab environment (e.g., File -> Upload to session storage).\n",
        "# 4. Run the following commands in a separate cell or uncomment and run them here:\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "# -----------------------------------------------------\n",
        "\n",
        "import kaggle\n",
        "\n",
        "# Download the entire dataset to a local directory and unzip it\n",
        "dataset_name = \"ashery/chexpert\"\n",
        "download_path = \"./CheXpert-v1.0-small\"\n",
        "kaggle.api.dataset_download_files(dataset_name, path=download_path, unzip=True)\n",
        "\n",
        "print(f\"Dataset '{dataset_name}' downloaded to: {download_path}\")\n",
        "print(\"Files in the downloaded dataset:\")\n",
        "!ls -F {download_path}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2XLOINtNhyP",
        "outputId": "1c79874a-916d-47e9-b65a-6880c68fc409"
      },
      "id": "Y2XLOINtNhyP",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.3.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.11.12)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n",
            "Dataset URL: https://www.kaggle.com/datasets/ashery/chexpert\n",
            "Dataset 'ashery/chexpert' downloaded to: ./CheXpert-v1.0-small\n",
            "Files in the downloaded dataset:\n",
            "train/\ttrain.csv  valid/  valid.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ff99391f",
      "metadata": {
        "id": "ff99391f"
      },
      "outputs": [],
      "source": [
        "# Load in data and then use only 50% for training and validation\n",
        "train_full = pd.read_csv(\"/content/CheXpert-v1.0-small/train.csv\")\n",
        "val_full = pd.read_csv(\"/content/CheXpert-v1.0-small/valid.csv\")\n",
        "train_subset = train_full.sample(frac=0.5, random_state=42)\n",
        "val_subset = val_full.sample(frac=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6b8bf01d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b8bf01d",
        "outputId": "60085b23-b27e-49df-9eaf-e7a6ef9f9efd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Path', 'Sex', 'Age', 'Frontal/Lateral', 'AP/PA', 'No Finding',\n",
              "       'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
              "       'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
              "       'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture',\n",
              "       'Support Devices'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "train_full.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "61531583",
      "metadata": {
        "id": "61531583"
      },
      "outputs": [],
      "source": [
        "train_full = train_full.fillna(0)\n",
        "val_full = val_full.fillna(0)\n",
        "train_subset = train_subset.fillna(0)\n",
        "val_subset = val_subset.fillna(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2775af5",
      "metadata": {
        "id": "f2775af5"
      },
      "source": [
        "Encoder Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1a38716",
      "metadata": {
        "id": "f1a38716"
      },
      "source": [
        "### Text Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b06a3783",
      "metadata": {
        "id": "b06a3783"
      },
      "outputs": [],
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim=512):\n",
        "        super(TextEncoder, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(\n",
        "            \"bert-base-uncased\",\n",
        "            output_hidden_states=True,\n",
        "        )\n",
        "        self.proj = nn.Linear(768, embed_dim)\n",
        "\n",
        "    def forward(self, token_ids, attention_masks):\n",
        "        outputs = self.bert(token_ids, attention_mask=attention_masks)\n",
        "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "        embeddings = self.proj(cls_embeddings)\n",
        "        # normalizing because we need to compare with image embeddings later\n",
        "        # for the contrastive similarity\n",
        "        embeddings = F.normalize(embeddings, p=2, dim=-1, eps=1e-6)\n",
        "        return embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5bc8a2c",
      "metadata": {
        "id": "b5bc8a2c"
      },
      "source": [
        "### Image Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7f97324d",
      "metadata": {
        "id": "7f97324d"
      },
      "outputs": [],
      "source": [
        "# Use the torchvision's implementation of ResNeXt, but add FC layer to generate 512d embedding.\n",
        "class VisionEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim=512):\n",
        "        super().__init__()\n",
        "        resnet = models.resnext50_32x4d(pretrained=True)\n",
        "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "        in_dim = resnet.fc.in_features\n",
        "        self.proj = nn.Linear(in_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        features = features.squeeze(-1).squeeze(-1)\n",
        "        z = self.proj(features)\n",
        "        # convert to unit vectors for cosine similarity later\n",
        "        z = F.normalize(z, p=2, dim=-1, eps=1e-6)\n",
        "        return z"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51c4d62f",
      "metadata": {
        "id": "51c4d62f"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b450ac04",
      "metadata": {
        "id": "b450ac04"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df):\n",
        "        df = df.reset_index(drop=True) # Reset index to ensure 0-based indexing\n",
        "        # Text\n",
        "        self.text_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "        self.reports = df.apply(\n",
        "            generate_report, axis=1\n",
        "        )\n",
        "\n",
        "        # Vision\n",
        "        self.images = df[\"Path\"]\n",
        "        self.transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize((256, 256)),\n",
        "                transforms.ToTensor(),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reports)  # This could work or we could do another way\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Text part\n",
        "        report = self.reports[idx]\n",
        "        encoder = self.text_tokenizer.encode_plus(\n",
        "            report,\n",
        "            max_length=512,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        # Vision part\n",
        "        img_path = self.images[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        img_tensor = self.transform(image)\n",
        "\n",
        "        return {\n",
        "            \"token_ids\": encoder[\"input_ids\"],\n",
        "            \"attention_masks\": encoder[\"attention_mask\"],\n",
        "            \"image_tensor\": img_tensor,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87b6818e",
      "metadata": {
        "id": "87b6818e"
      },
      "source": [
        "Samples come in as dataframe and we put in dataset. For each sample, we can then return inputs for text encoder (token ids & attention masks) & for vision encoder (tensor of pixels). We pass relevant inputs to text encoder and vision encoder, get embeddings for that sample. Then blah blah"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be3bf8db",
      "metadata": {
        "id": "be3bf8db"
      },
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "201af40b",
      "metadata": {
        "id": "201af40b"
      },
      "outputs": [],
      "source": [
        "def generate_report(row):\n",
        "    labels = row.iloc[4:]\n",
        "    findings_list = list(labels[labels == 1].index)\n",
        "    findings = \", \".join(findings_list)\n",
        "    return f\"X-Ray report findings: {findings}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6213f6c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272,
          "referenced_widgets": [
            "a250e3a658694acabee3754dbcdce981",
            "4b0576d077eb4535a7458489e3ab2338",
            "61ad6014b38f4950abaddc2c8048aea4",
            "7ff0c2b01fba44aca8563b9b2d86d8c0",
            "f4f23c759ad7442c9bcd31ab77794a7a",
            "b6b9480588474f9a92ed7977997f2058",
            "772be23f459b4fa2b8629acea64630bb",
            "ed623b0d22ac49239c1e710602ce3caf",
            "df6346050a8a4c36ae4a952595997787",
            "3dd3c22733704be9a8d2c7915d7362d2",
            "b8227d1aaddb42e7aaca4e46e2ef1feb",
            "85d28fa4f8b04f2ebc7c0e25b7ae9849",
            "0574df8bceb94f80b069013dc5933540",
            "e9d473dd4bcb4ceea747edf2d79c1c1f",
            "ec74d49d36e842f5a035540632c6c551",
            "b9cee010b25c46d1ad25c0bbe845e009",
            "9eb0ab9585114fbb984bb12e5c74cb12",
            "4ec2fa8d1bda4e1894fa9b732139b157",
            "0256cc8a00a64dad8930c50834c6cf68",
            "4aa54350339c46d88bdd15b6f2a751e6",
            "280ae84c88a0426e94efcdbcd3a6d2af",
            "6ec1ed7796b04d4d99cf068d3e86d379",
            "5ab5a1e1caa54eb3a43a436246dedd73",
            "838803d285a149ae85c0d0446d0a4707",
            "95f6d7e5dde1484ca91305ce0cae343d",
            "84ac86a997d7473e8dde41fd12a85ad0",
            "84795388befa40968fe6b879ab6313ae",
            "215529a8aaad45bfb9fc834ad0cb5602",
            "0a45c9f035854c5a938dfa784b130dc9",
            "2c77b74d62a04e9bbb998bbe52124d53",
            "53b39fa0200748be872b735e0f22f494",
            "f9de1c5b59f94520a7cbe16f55c11f70",
            "b0b9da5d7e0644f696da7563a8257206",
            "606a15c383ca473cb3b8bdde7ec165d2",
            "184ab117c899407f804b1c7e5413e8c0",
            "38648b003c1d47ad8e5acc806385fb41",
            "dff8aeb957ac4d0398db8c80cce2cb91",
            "1927734c112242778f39340d1836b28c",
            "e304d09d7ee84816be0bf9e9d288c6cf",
            "b9f47a76ce9743e9ad9c2f68fb191799",
            "2d54c8ea278941718d76f7381e2ba185",
            "99222e948ea34487aac84be4637b7f06",
            "c846b039610b430e8d93ab640bb5514f",
            "f4b58bed9a394631963cd34defe06774"
          ]
        },
        "id": "6213f6c7",
        "outputId": "9ef165ff-5b01-4866-f22c-1ccb07350975"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a250e3a658694acabee3754dbcdce981"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85d28fa4f8b04f2ebc7c0e25b7ae9849"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ab5a1e1caa54eb3a43a436246dedd73"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "606a15c383ca473cb3b8bdde7ec165d2"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Create train & val datasets\n",
        "train_dataset = CustomDataset(train_subset)\n",
        "val_dataset = CustomDataset(val_subset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "377f05ca",
      "metadata": {
        "id": "377f05ca"
      },
      "outputs": [],
      "source": [
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abf933c7",
      "metadata": {
        "id": "abf933c7"
      },
      "source": [
        "### Contrastive Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3fba9bd",
      "metadata": {
        "id": "d3fba9bd"
      },
      "source": [
        "# image_encoder - ResNet or Vision Transformer\n",
        "# text_encoder - CBOW or Text Transformer\n",
        "# I[n, h, w, c] - minibatch of aligned images\n",
        "K: n is batch size, h,w is height width, c is number of channels (3 for RGB)\n",
        "\n",
        "# T[n, l] - minibatch of aligned texts\n",
        "K: n is batch size, l is sequence length (number of tokens)\n",
        "\n",
        "K: aligned means that they are paired up\n",
        "\n",
        "# W_i[d_i, d_e] - learned proj of image to embed\n",
        "# W_t[d_t, d_e] - learned proj of text to embed\n",
        "# t - learned temperature parameter\n",
        "\n",
        "# extract feature representations of each modality\n",
        "I_f = image_encoder(I) #[n, d_i]\n",
        "this happens in vision_enc\n",
        "K: get features from resnet\n",
        "\n",
        "T_f = text_encoder(T) #[n, d_t]\n",
        "this happens in text_enc\n",
        "K: get features from bert\n",
        "\n",
        "# joint multimodal embedding [n, d_e]\n",
        "K: this is projecting to joint embedding space\n",
        "I_e = l2_normalize(np.dot(I_f, W_i), axis=1)\n",
        "T_e = l2_normalize(np.dot(T_f, W_t), axis=1)\n",
        "\n",
        "# scaled pairwise cosine similarities [n, n]\n",
        "logits = np.dot(I_e, T_e.T) * np.exp(t) #K: scale by temperature\n",
        "\n",
        "\n",
        "# symmetric loss function\n",
        "labels = np.arange(n) #K: correct match is on the diagonal\n",
        "# K: now we compute symmetric loss\n",
        "loss_i = cross_entropy_loss(logits, labels, axis=0) #K: image to text\n",
        "loss_t = cross_entropy_loss(logits, labels, axis=1) #K: text to image\n",
        "# K: now average in both directions\n",
        "loss = (loss_i + loss_t)/2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "16f8dc1c",
      "metadata": {
        "id": "16f8dc1c"
      },
      "outputs": [],
      "source": [
        "# I_f, T_f, I_e, T_e are all done in other places\n",
        "def contrastive_loss(image_embeddings, text_embeddings, temperature=0.07):\n",
        "    # we can make the t a fixed parameter but most CLIP reproductions make it fixed\n",
        "    # so just keeping it as argument for now\n",
        "\n",
        "    batch_size = image_embeddings.shape[0]  # called n in the pseudo code\n",
        "\n",
        "    # we already have normalized embeddings, so we can skip to cosine similarity part of pseudo code\n",
        "    # \"scaled pairwise cosine similarities [n, n]\"\n",
        "    # dividing by temperature to scale the logits\n",
        "    logits = torch.matmul(image_embeddings, text_embeddings.T) / temperature\n",
        "    # now \"symmetric loss function\"\n",
        "    labels = torch.arange(batch_size, device=image_embeddings.device)\n",
        "    loss_img_to_text = F.cross_entropy(logits, labels)\n",
        "    loss_text_to_img = F.cross_entropy(logits.T, labels)\n",
        "\n",
        "    # average loss in both directions\n",
        "    loss = (loss_img_to_text + loss_text_to_img) / 2\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "SAVE_PATH = \"/content/drive/MyDrive/models/vision_text_new/\"\n",
        "\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nn4IcN6QfqFn",
        "outputId": "5c781dfe-2ccd-46c3-8f79-89e2ed47c8ca"
      },
      "id": "nn4IcN6QfqFn",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def recall_at_k(image_embeds, text_embeds, k_values=[1, 5]):\n",
        "    \"\"\"\n",
        "    Compute Recall@K for image-to-text and text-to-image retrieval\n",
        "    \"\"\"\n",
        "    # Compute similarity matrix: [N_images, N_texts]\n",
        "    similarity = torch.matmul(image_embeds, text_embeds.T)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Image-to-Text Recall@K\n",
        "    for k in k_values:\n",
        "        # For each image, get top-k most similar texts\n",
        "        top_k_indices = similarity.topk(k, dim=1).indices  # [N, k]\n",
        "\n",
        "        # Check if correct text (same index) is in top-k\n",
        "        correct = torch.zeros(len(image_embeds))\n",
        "        for i in range(len(image_embeds)):\n",
        "            if i in top_k_indices[i]:\n",
        "                correct[i] = 1\n",
        "\n",
        "        recall = correct.mean().item()\n",
        "        results[f\"image_to_text_recall@{k}\"] = recall\n",
        "\n",
        "    # Text-to-Image Recall@K\n",
        "    for k in k_values:\n",
        "        # For each text, get top-k most similar images\n",
        "        top_k_indices = similarity.T.topk(k, dim=1).indices  # [N, k]\n",
        "\n",
        "        # Check if correct image (same index) is in top-k\n",
        "        correct = torch.zeros(len(text_embeds))\n",
        "        for i in range(len(text_embeds)):\n",
        "            if i in top_k_indices[i]:\n",
        "                correct[i] = 1\n",
        "\n",
        "        recall = correct.mean().item()\n",
        "        results[f\"text_to_image_recall@{k}\"] = recall\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "qkSM8-n0GhjL"
      },
      "id": "qkSM8-n0GhjL",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "0f93aec2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "472f62dc31874c328a8a3526408063b5",
            "d1ec631f8ff14da0b980920a72c190f1",
            "890f529f896c4208af7b57a2169efc6b",
            "4ed491961fd948d6bb89e12334a26606",
            "2ea20d3f74bd48c399b0d64196af3cbd",
            "d0fd2cda66da440fac413f2ed6b6b04c",
            "9c758db06e3b4b1ca3cf75f29a14d95a",
            "1ff9c897f2744c2cacbc6c62e999c163",
            "f3096054248a4ca0b2e58c37aa901b84",
            "4f4f344ef47f446cbcc7e9d7bff9e88f",
            "12c4f2dfeaa14a6e8e7a61c658bccd3c"
          ]
        },
        "id": "0f93aec2",
        "outputId": "f5b121b5-e4f6-4def-a2d3-d13b0da74868"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "472f62dc31874c328a8a3526408063b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt50_32X4D_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\" to /root/.cache/torch/hub/checkpoints/resnext50_32x4d-7cdf4587.pth\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 95.8M/95.8M [00:00<00:00, 203MB/s]\n",
            "/tmp/ipython-input-363749945.py:39: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "/tmp/ipython-input-363749945.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "/tmp/ipython-input-363749945.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 iter:   0 val loss:2.649, recall@k: {'image_to_text_recall@1': 0.00855, 'image_to_text_recall@5': 0.04274, 'text_to_image_recall@1': 0.00855, 'text_to_image_recall@5': 0.05128}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_0.pt\n",
            "epoch: 1 iter: 500 val loss:2.260, recall@k: {'image_to_text_recall@1': 0.04274, 'image_to_text_recall@5': 0.09402, 'text_to_image_recall@1': 0.03419, 'text_to_image_recall@5': 0.12821}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_500.pt\n",
            "epoch: 1 iter:1000 val loss:2.198, recall@k: {'image_to_text_recall@1': 0.04274, 'image_to_text_recall@5': 0.1453, 'text_to_image_recall@1': 0.03419, 'text_to_image_recall@5': 0.21368}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_1000.pt\n",
            "epoch: 1 iter:1500 val loss:2.113, recall@k: {'image_to_text_recall@1': 0.02564, 'image_to_text_recall@5': 0.16239, 'text_to_image_recall@1': 0.02564, 'text_to_image_recall@5': 0.18803}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_1500.pt\n",
            "epoch: 1 iter:2000 val loss:2.071, recall@k: {'image_to_text_recall@1': 0.00855, 'image_to_text_recall@5': 0.12821, 'text_to_image_recall@1': 0.04274, 'text_to_image_recall@5': 0.21368}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_2000.pt\n",
            "epoch: 1 iter:2500 val loss:1.877, recall@k: {'image_to_text_recall@1': 0.03419, 'image_to_text_recall@5': 0.17094, 'text_to_image_recall@1': 0.04274, 'text_to_image_recall@5': 0.19658}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_2500.pt\n",
            "epoch: 1 iter:3000 val loss:1.925, recall@k: {'image_to_text_recall@1': 0.05983, 'image_to_text_recall@5': 0.17949, 'text_to_image_recall@1': 0.08547, 'text_to_image_recall@5': 0.23932}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_3000.pt\n",
            "epoch: 1 iter:3500 val loss:1.914, recall@k: {'image_to_text_recall@1': 0.04274, 'image_to_text_recall@5': 0.19658, 'text_to_image_recall@1': 0.05128, 'text_to_image_recall@5': 0.23077}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_3500.pt\n",
            "epoch: 1 iter:4000 val loss:1.854, recall@k: {'image_to_text_recall@1': 0.05983, 'image_to_text_recall@5': 0.19658, 'text_to_image_recall@1': 0.05983, 'text_to_image_recall@5': 0.25641}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_4000.pt\n",
            "epoch: 1 iter:4500 val loss:1.851, recall@k: {'image_to_text_recall@1': 0.05983, 'image_to_text_recall@5': 0.19658, 'text_to_image_recall@1': 0.03419, 'text_to_image_recall@5': 0.24786}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_4500.pt\n",
            "epoch: 1 iter:5000 val loss:1.846, recall@k: {'image_to_text_recall@1': 0.04274, 'image_to_text_recall@5': 0.23932, 'text_to_image_recall@1': 0.08547, 'text_to_image_recall@5': 0.26496}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_5000.pt\n",
            "epoch: 1 iter:5500 val loss:2.056, recall@k: {'image_to_text_recall@1': 0.05128, 'image_to_text_recall@5': 0.17094, 'text_to_image_recall@1': 0.05128, 'text_to_image_recall@5': 0.23077}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_5500.pt\n",
            "epoch: 1 iter:6000 val loss:1.938, recall@k: {'image_to_text_recall@1': 0.06838, 'image_to_text_recall@5': 0.22222, 'text_to_image_recall@1': 0.03419, 'text_to_image_recall@5': 0.23932}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_6000.pt\n",
            "epoch: 1 iter:6500 val loss:1.958, recall@k: {'image_to_text_recall@1': 0.06838, 'image_to_text_recall@5': 0.19658, 'text_to_image_recall@1': 0.07692, 'text_to_image_recall@5': 0.23932}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_6500.pt\n",
            "epoch: 1 iter:6982 train loss:2.149\n",
            "\n",
            "epoch: 2 iter:7000 val loss:1.955, recall@k: {'image_to_text_recall@1': 0.05128, 'image_to_text_recall@5': 0.19658, 'text_to_image_recall@1': 0.08547, 'text_to_image_recall@5': 0.26496}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_7000.pt\n",
            "epoch: 2 iter:7500 val loss:1.851, recall@k: {'image_to_text_recall@1': 0.04274, 'image_to_text_recall@5': 0.23077, 'text_to_image_recall@1': 0.09402, 'text_to_image_recall@5': 0.2735}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_7500.pt\n",
            "epoch: 2 iter:8000 val loss:1.850, recall@k: {'image_to_text_recall@1': 0.07692, 'image_to_text_recall@5': 0.29915, 'text_to_image_recall@1': 0.09402, 'text_to_image_recall@5': 0.28205}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_8000.pt\n",
            "epoch: 2 iter:8500 val loss:1.709, recall@k: {'image_to_text_recall@1': 0.06838, 'image_to_text_recall@5': 0.24786, 'text_to_image_recall@1': 0.03419, 'text_to_image_recall@5': 0.2906}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_8500.pt\n",
            "epoch: 2 iter:9000 val loss:1.834, recall@k: {'image_to_text_recall@1': 0.06838, 'image_to_text_recall@5': 0.22222, 'text_to_image_recall@1': 0.07692, 'text_to_image_recall@5': 0.2735}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_9000.pt\n",
            "epoch: 2 iter:9500 val loss:1.679, recall@k: {'image_to_text_recall@1': 0.07692, 'image_to_text_recall@5': 0.24786, 'text_to_image_recall@1': 0.08547, 'text_to_image_recall@5': 0.26496}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_9500.pt\n",
            "epoch: 2 iter:10000 val loss:1.823, recall@k: {'image_to_text_recall@1': 0.05128, 'image_to_text_recall@5': 0.2906, 'text_to_image_recall@1': 0.07692, 'text_to_image_recall@5': 0.32479}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_10000.pt\n",
            "epoch: 2 iter:10500 val loss:1.793, recall@k: {'image_to_text_recall@1': 0.05128, 'image_to_text_recall@5': 0.24786, 'text_to_image_recall@1': 0.06838, 'text_to_image_recall@5': 0.28205}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_10500.pt\n",
            "epoch: 2 iter:11000 val loss:1.818, recall@k: {'image_to_text_recall@1': 0.07692, 'image_to_text_recall@5': 0.2735, 'text_to_image_recall@1': 0.06838, 'text_to_image_recall@5': 0.30769}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_11000.pt\n",
            "epoch: 2 iter:11500 val loss:1.958, recall@k: {'image_to_text_recall@1': 0.05983, 'image_to_text_recall@5': 0.18803, 'text_to_image_recall@1': 0.06838, 'text_to_image_recall@5': 0.26496}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_11500.pt\n",
            "epoch: 2 iter:12000 val loss:1.928, recall@k: {'image_to_text_recall@1': 0.05983, 'image_to_text_recall@5': 0.23932, 'text_to_image_recall@1': 0.08547, 'text_to_image_recall@5': 0.2735}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_12000.pt\n",
            "epoch: 2 iter:12500 val loss:1.864, recall@k: {'image_to_text_recall@1': 0.05983, 'image_to_text_recall@5': 0.23932, 'text_to_image_recall@1': 0.08547, 'text_to_image_recall@5': 0.26496}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_12500.pt\n",
            "epoch: 2 iter:13000 val loss:1.910, recall@k: {'image_to_text_recall@1': 0.04274, 'image_to_text_recall@5': 0.20513, 'text_to_image_recall@1': 0.03419, 'text_to_image_recall@5': 0.23077}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_13000.pt\n",
            "epoch: 2 iter:13500 val loss:1.995, recall@k: {'image_to_text_recall@1': 0.06838, 'image_to_text_recall@5': 0.26496, 'text_to_image_recall@1': 0.04274, 'text_to_image_recall@5': 0.22222}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_13500.pt\n",
            "epoch: 2 iter:13964 train loss:1.946\n",
            "\n",
            "epoch: 3 iter:14000 val loss:1.763, recall@k: {'image_to_text_recall@1': 0.06838, 'image_to_text_recall@5': 0.2735, 'text_to_image_recall@1': 0.08547, 'text_to_image_recall@5': 0.2906}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_14000.pt\n",
            "epoch: 3 iter:14500 val loss:1.718, recall@k: {'image_to_text_recall@1': 0.06838, 'image_to_text_recall@5': 0.2906, 'text_to_image_recall@1': 0.07692, 'text_to_image_recall@5': 0.32479}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_14500.pt\n",
            "epoch: 3 iter:15000 val loss:1.777, recall@k: {'image_to_text_recall@1': 0.05128, 'image_to_text_recall@5': 0.25641, 'text_to_image_recall@1': 0.04274, 'text_to_image_recall@5': 0.23077}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_15000.pt\n",
            "epoch: 3 iter:15500 val loss:1.864, recall@k: {'image_to_text_recall@1': 0.06838, 'image_to_text_recall@5': 0.19658, 'text_to_image_recall@1': 0.03419, 'text_to_image_recall@5': 0.19658}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_15500.pt\n",
            "epoch: 3 iter:16000 val loss:1.821, recall@k: {'image_to_text_recall@1': 0.05128, 'image_to_text_recall@5': 0.20513, 'text_to_image_recall@1': 0.03419, 'text_to_image_recall@5': 0.23932}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_16000.pt\n",
            "epoch: 3 iter:16500 val loss:1.820, recall@k: {'image_to_text_recall@1': 0.05128, 'image_to_text_recall@5': 0.23077, 'text_to_image_recall@1': 0.05983, 'text_to_image_recall@5': 0.23932}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_16500.pt\n",
            "epoch: 3 iter:17000 val loss:1.801, recall@k: {'image_to_text_recall@1': 0.05128, 'image_to_text_recall@5': 0.23077, 'text_to_image_recall@1': 0.07692, 'text_to_image_recall@5': 0.25641}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_17000.pt\n",
            "epoch: 3 iter:17500 val loss:1.959, recall@k: {'image_to_text_recall@1': 0.06838, 'image_to_text_recall@5': 0.23932, 'text_to_image_recall@1': 0.07692, 'text_to_image_recall@5': 0.23932}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_17500.pt\n",
            "epoch: 3 iter:18000 val loss:1.793, recall@k: {'image_to_text_recall@1': 0.05983, 'image_to_text_recall@5': 0.2735, 'text_to_image_recall@1': 0.08547, 'text_to_image_recall@5': 0.28205}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_18000.pt\n",
            "epoch: 3 iter:18500 val loss:1.843, recall@k: {'image_to_text_recall@1': 0.11111, 'image_to_text_recall@5': 0.22222, 'text_to_image_recall@1': 0.06838, 'text_to_image_recall@5': 0.26496}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_18500.pt\n",
            "epoch: 3 iter:19000 val loss:1.948, recall@k: {'image_to_text_recall@1': 0.06838, 'image_to_text_recall@5': 0.19658, 'text_to_image_recall@1': 0.04274, 'text_to_image_recall@5': 0.23077}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_19000.pt\n",
            "epoch: 3 iter:19500 val loss:1.899, recall@k: {'image_to_text_recall@1': 0.05128, 'image_to_text_recall@5': 0.18803, 'text_to_image_recall@1': 0.07692, 'text_to_image_recall@5': 0.17094}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_19500.pt\n",
            "epoch: 3 iter:20000 val loss:2.122, recall@k: {'image_to_text_recall@1': 0.06838, 'image_to_text_recall@5': 0.15385, 'text_to_image_recall@1': 0.03419, 'text_to_image_recall@5': 0.25641}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_20000.pt\n",
            "epoch: 3 iter:20500 val loss:2.050, recall@k: {'image_to_text_recall@1': 0.05128, 'image_to_text_recall@5': 0.21368, 'text_to_image_recall@1': 0.05128, 'text_to_image_recall@5': 0.22222}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_20500.pt\n",
            "epoch: 3 iter:20946 train loss:1.759\n",
            "\n",
            "epoch: 4 iter:21000 val loss:1.848, recall@k: {'image_to_text_recall@1': 0.08547, 'image_to_text_recall@5': 0.23077, 'text_to_image_recall@1': 0.05983, 'text_to_image_recall@5': 0.2735}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_21000.pt\n",
            "epoch: 4 iter:21500 val loss:1.812, recall@k: {'image_to_text_recall@1': 0.06838, 'image_to_text_recall@5': 0.23932, 'text_to_image_recall@1': 0.06838, 'text_to_image_recall@5': 0.2735}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_21500.pt\n",
            "epoch: 4 iter:22000 val loss:2.011, recall@k: {'image_to_text_recall@1': 0.06838, 'image_to_text_recall@5': 0.18803, 'text_to_image_recall@1': 0.04274, 'text_to_image_recall@5': 0.23077}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_22000.pt\n",
            "epoch: 4 iter:22500 val loss:1.896, recall@k: {'image_to_text_recall@1': 0.05983, 'image_to_text_recall@5': 0.26496, 'text_to_image_recall@1': 0.04274, 'text_to_image_recall@5': 0.31624}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_22500.pt\n",
            "epoch: 4 iter:23000 val loss:1.953, recall@k: {'image_to_text_recall@1': 0.06838, 'image_to_text_recall@5': 0.21368, 'text_to_image_recall@1': 0.03419, 'text_to_image_recall@5': 0.24786}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_23000.pt\n",
            "epoch: 4 iter:23500 val loss:1.950, recall@k: {'image_to_text_recall@1': 0.04274, 'image_to_text_recall@5': 0.23932, 'text_to_image_recall@1': 0.05983, 'text_to_image_recall@5': 0.23932}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_23500.pt\n",
            "epoch: 4 iter:24000 val loss:1.861, recall@k: {'image_to_text_recall@1': 0.07692, 'image_to_text_recall@5': 0.20513, 'text_to_image_recall@1': 0.05983, 'text_to_image_recall@5': 0.24786}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_24000.pt\n",
            "epoch: 4 iter:24500 val loss:1.964, recall@k: {'image_to_text_recall@1': 0.07692, 'image_to_text_recall@5': 0.23077, 'text_to_image_recall@1': 0.05128, 'text_to_image_recall@5': 0.25641}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_24500.pt\n",
            "epoch: 4 iter:25000 val loss:1.929, recall@k: {'image_to_text_recall@1': 0.08547, 'image_to_text_recall@5': 0.2906, 'text_to_image_recall@1': 0.09402, 'text_to_image_recall@5': 0.26496}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_25000.pt\n",
            "epoch: 4 iter:25500 val loss:2.078, recall@k: {'image_to_text_recall@1': 0.08547, 'image_to_text_recall@5': 0.26496, 'text_to_image_recall@1': 0.05983, 'text_to_image_recall@5': 0.28205}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_25500.pt\n",
            "epoch: 4 iter:26000 val loss:2.052, recall@k: {'image_to_text_recall@1': 0.05128, 'image_to_text_recall@5': 0.22222, 'text_to_image_recall@1': 0.05128, 'text_to_image_recall@5': 0.25641}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_26000.pt\n",
            "epoch: 4 iter:26500 val loss:2.127, recall@k: {'image_to_text_recall@1': 0.05128, 'image_to_text_recall@5': 0.25641, 'text_to_image_recall@1': 0.04274, 'text_to_image_recall@5': 0.21368}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_26500.pt\n",
            "epoch: 4 iter:27000 val loss:2.289, recall@k: {'image_to_text_recall@1': 0.04274, 'image_to_text_recall@5': 0.18803, 'text_to_image_recall@1': 0.03419, 'text_to_image_recall@5': 0.21368}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_27000.pt\n",
            "epoch: 4 iter:27500 val loss:2.179, recall@k: {'image_to_text_recall@1': 0.06838, 'image_to_text_recall@5': 0.22222, 'text_to_image_recall@1': 0.05128, 'text_to_image_recall@5': 0.21368}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_27500.pt\n",
            "epoch: 4 iter:27928 train loss:1.524\n",
            "\n",
            "epoch: 5 iter:28000 val loss:2.214, recall@k: {'image_to_text_recall@1': 0.04274, 'image_to_text_recall@5': 0.19658, 'text_to_image_recall@1': 0.02564, 'text_to_image_recall@5': 0.23077}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_28000.pt\n",
            "epoch: 5 iter:28500 val loss:1.997, recall@k: {'image_to_text_recall@1': 0.08547, 'image_to_text_recall@5': 0.25641, 'text_to_image_recall@1': 0.05983, 'text_to_image_recall@5': 0.24786}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_28500.pt\n",
            "epoch: 5 iter:29000 val loss:2.178, recall@k: {'image_to_text_recall@1': 0.06838, 'image_to_text_recall@5': 0.23077, 'text_to_image_recall@1': 0.02564, 'text_to_image_recall@5': 0.23932}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_29000.pt\n",
            "epoch: 5 iter:29500 val loss:2.125, recall@k: {'image_to_text_recall@1': 0.06838, 'image_to_text_recall@5': 0.20513, 'text_to_image_recall@1': 0.01709, 'text_to_image_recall@5': 0.25641}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_29500.pt\n",
            "epoch: 5 iter:30000 val loss:2.106, recall@k: {'image_to_text_recall@1': 0.07692, 'image_to_text_recall@5': 0.20513, 'text_to_image_recall@1': 0.05128, 'text_to_image_recall@5': 0.23932}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_30000.pt\n",
            "epoch: 5 iter:30500 val loss:2.140, recall@k: {'image_to_text_recall@1': 0.04274, 'image_to_text_recall@5': 0.20513, 'text_to_image_recall@1': 0.05128, 'text_to_image_recall@5': 0.23077}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_30500.pt\n",
            "epoch: 5 iter:31000 val loss:2.250, recall@k: {'image_to_text_recall@1': 0.05128, 'image_to_text_recall@5': 0.22222, 'text_to_image_recall@1': 0.04274, 'text_to_image_recall@5': 0.21368}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_31000.pt\n",
            "epoch: 5 iter:31500 val loss:2.431, recall@k: {'image_to_text_recall@1': 0.03419, 'image_to_text_recall@5': 0.17949, 'text_to_image_recall@1': 0.05983, 'text_to_image_recall@5': 0.18803}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_31500.pt\n",
            "epoch: 5 iter:32000 val loss:2.328, recall@k: {'image_to_text_recall@1': 0.07692, 'image_to_text_recall@5': 0.18803, 'text_to_image_recall@1': 0.05128, 'text_to_image_recall@5': 0.22222}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_32000.pt\n",
            "epoch: 5 iter:32500 val loss:2.291, recall@k: {'image_to_text_recall@1': 0.07692, 'image_to_text_recall@5': 0.20513, 'text_to_image_recall@1': 0.05128, 'text_to_image_recall@5': 0.23077}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_32500.pt\n",
            "epoch: 5 iter:33000 val loss:2.124, recall@k: {'image_to_text_recall@1': 0.05128, 'image_to_text_recall@5': 0.21368, 'text_to_image_recall@1': 0.04274, 'text_to_image_recall@5': 0.28205}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_33000.pt\n",
            "epoch: 5 iter:33500 val loss:2.654, recall@k: {'image_to_text_recall@1': 0.05128, 'image_to_text_recall@5': 0.17949, 'text_to_image_recall@1': 0.06838, 'text_to_image_recall@5': 0.20513}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_33500.pt\n",
            "epoch: 5 iter:34000 val loss:2.740, recall@k: {'image_to_text_recall@1': 0.02564, 'image_to_text_recall@5': 0.17949, 'text_to_image_recall@1': 0.05128, 'text_to_image_recall@5': 0.17094}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_34000.pt\n",
            "epoch: 5 iter:34500 val loss:2.479, recall@k: {'image_to_text_recall@1': 0.05983, 'image_to_text_recall@5': 0.18803, 'text_to_image_recall@1': 0.02564, 'text_to_image_recall@5': 0.22222}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_new/checkpoint_iter_34500.pt\n",
            "epoch: 5 iter:34910 train loss:1.294\n",
            "\n",
            "Final checkpoint saved to /content/final_checkpoint.pt\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Setup\n",
        "# =========================\n",
        "temperature = 0.07\n",
        "max_epoch_number = 5\n",
        "test_freq = 500            # validation frequency\n",
        "ckpt_freq = 500            # checkpoint frequency (iterations)\n",
        "\n",
        "# Initialize models\n",
        "text_enc = TextEncoder(embed_dim=512).to(device)\n",
        "vision_enc = VisionEncoder(embed_dim=512).to(device)\n",
        "\n",
        "# Optimizer\n",
        "# Use differentiated learning rates (THIS IS CRITICAL!)\n",
        "lr_bert = 2e-5  # Much smaller for pretrained BERT\n",
        "lr_other = 1e-4  # Normal for linear layers and vision encoder\n",
        "\n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': text_enc.bert.parameters(), 'lr': lr_bert},\n",
        "    {'params': text_enc.proj.parameters(), 'lr': lr_other},\n",
        "    {'params': vision_enc.parameters(), 'lr': lr_other}\n",
        "])\n",
        "\n",
        "# Freeze BERT + ResNet weights\n",
        "# for p in text_enc.bert.parameters():\n",
        "#     p.requires_grad = False\n",
        "\n",
        "# for p in vision_enc.backbone.parameters():\n",
        "#     p.requires_grad = False\n",
        "\n",
        "# # Only projection layers train\n",
        "# proj_params = list(text_enc.proj.parameters()) + list(vision_enc.proj.parameters())\n",
        "# optimizer = torch.optim.Adam(proj_params, lr=1e-3)  # you can afford a higher LR here\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Mixed Precision Setup\n",
        "# =========================\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "iteration = 0\n",
        "for epoch in range(max_epoch_number):\n",
        "    text_enc.train()\n",
        "    vision_enc.train()\n",
        "    batch_losses = []\n",
        "\n",
        "    for batch in train_loader:\n",
        "        images = batch[\"image_tensor\"].to(device)\n",
        "        token_ids = batch[\"token_ids\"].squeeze(1).to(device)\n",
        "        attention_masks = batch[\"attention_masks\"].squeeze(1).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # =========================\n",
        "        # Forward with mixed precision\n",
        "        # =========================\n",
        "        with torch.cuda.amp.autocast():\n",
        "            image_embeddings = vision_enc(images)\n",
        "            text_embeddings = text_enc(token_ids, attention_masks)\n",
        "\n",
        "            if torch.isnan(image_embeddings).any():\n",
        "                print(\"NaN in image_embeddings\"); break\n",
        "            if torch.isnan(text_embeddings).any():\n",
        "                print(\"NaN in text_embeddings\"); break\n",
        "\n",
        "            loss = contrastive_loss(image_embeddings, text_embeddings, temperature)\n",
        "\n",
        "            if torch.isnan(loss).any():\n",
        "                print(\"NaN in loss\"); break\n",
        "\n",
        "        # =========================\n",
        "        # Backward with gradient scaling\n",
        "        # =========================\n",
        "        batch_loss_value = loss.item()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        batch_losses.append(batch_loss_value)\n",
        "\n",
        "        # =========================\n",
        "        # Validation\n",
        "        # =========================\n",
        "        if iteration % test_freq == 0:\n",
        "            text_enc.eval()\n",
        "            vision_enc.eval()\n",
        "            val_losses = []\n",
        "            all_img_embeds = []\n",
        "            all_text_embeds = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch_val in val_loader:\n",
        "                    val_images = batch_val[\"image_tensor\"].to(device)\n",
        "                    val_token_ids = batch_val[\"token_ids\"].squeeze(1).to(device)\n",
        "                    val_attention_masks = batch_val[\"attention_masks\"].squeeze(1).to(device)\n",
        "\n",
        "                    # Mixed precision for validation too\n",
        "                    with torch.cuda.amp.autocast():\n",
        "                        val_img_embeds = vision_enc(val_images)\n",
        "                        val_txt_embeds = text_enc(val_token_ids, val_attention_masks)\n",
        "                        val_loss = contrastive_loss(val_img_embeds, val_txt_embeds, temperature)\n",
        "\n",
        "                    val_losses.append(val_loss.item())\n",
        "                    all_img_embeds.append(val_img_embeds.cpu())\n",
        "                    all_text_embeds.append(val_txt_embeds.cpu())\n",
        "\n",
        "            all_img_embeds = torch.cat(all_img_embeds, dim=0)   # shape (N, d)\n",
        "            all_text_embeds = torch.cat(all_text_embeds, dim=0)\n",
        "\n",
        "            avg_val_loss = float(np.mean(val_losses))\n",
        "            recall_results = recall_at_k(all_img_embeds, all_text_embeds)\n",
        "            rounded_recall_results = {key: round(value, 5) for key, value in recall_results.items()}\n",
        "            print(f\"epoch:{epoch+1:2d} iter:{iteration:4d} val loss:{avg_val_loss:.3f}, recall@k: {rounded_recall_results}\")\n",
        "\n",
        "            text_enc.train()\n",
        "            vision_enc.train()\n",
        "\n",
        "        # =========================\n",
        "        # Periodic checkpoint\n",
        "        # =========================\n",
        "        if iteration % ckpt_freq == 0:\n",
        "            ckpt_path = os.path.join(SAVE_PATH, f\"checkpoint_iter_{iteration}.pt\")\n",
        "            torch.save(\n",
        "                {\n",
        "                    \"epoch\": epoch,\n",
        "                    \"iteration\": iteration,\n",
        "                    \"text_enc\": text_enc.state_dict(),\n",
        "                    \"vision_enc\": vision_enc.state_dict(),\n",
        "                    \"optimizer\": optimizer.state_dict(),\n",
        "                    \"scaler\": scaler.state_dict(),  # Save scaler state\n",
        "                    \"train_loss\": float(batch_loss_value),\n",
        "                },\n",
        "                ckpt_path,\n",
        "            )\n",
        "            print(f\"Checkpoint saved to {ckpt_path}\")\n",
        "\n",
        "        iteration += 1\n",
        "\n",
        "    train_loss = float(np.mean(batch_losses))\n",
        "    print(f\"epoch:{epoch+1:2d} iter:{iteration:4d} train loss:{train_loss:.3f}\\n\")\n",
        "\n",
        "# =========================\n",
        "# Final checkpoint\n",
        "# =========================\n",
        "final_path = os.path.join(\"/content/final_checkpoint.pt\")\n",
        "torch.save(\n",
        "    {\n",
        "        \"epoch\": epoch,\n",
        "        \"iteration\": iteration,\n",
        "        \"text_enc\": text_enc.state_dict(),\n",
        "        \"vision_enc\": vision_enc.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "        \"scaler\": scaler.state_dict(),  # Save scaler state\n",
        "        \"train_loss\": train_loss,\n",
        "    },\n",
        "    final_path,\n",
        ")\n",
        "print(f\"Final checkpoint saved to {final_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_path = os.path.join(SAVE_PATH, \"final_checkpoint.pt\")\n",
        "\n",
        "torch.save(\n",
        "    {\n",
        "        \"epoch\": epoch,\n",
        "        \"iteration\": iteration,\n",
        "        \"text_enc\": text_enc.state_dict(),\n",
        "        \"vision_enc\": vision_enc.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "        \"scaler\": scaler.state_dict(),  # Save scaler state\n",
        "        \"train_loss\": train_loss,\n",
        "    },\n",
        "    final_path,\n",
        ")\n",
        "print(f\"Final checkpoint saved to {final_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBFI0QPnr-Zq",
        "outputId": "b61a99aa-0c0e-4d88-9109-21359ae2c3e2"
      },
      "id": "YBFI0QPnr-Zq",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final checkpoint saved to /content/drive/MyDrive/models/vision_text_new/final_checkpoint.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continue Training on Full Dataset, With Adjusted LRs"
      ],
      "metadata": {
        "id": "Kxv9SKfMta8B"
      },
      "id": "Kxv9SKfMta8B"
    },
    {
      "cell_type": "code",
      "source": [
        "train_full = train_full.drop(\"AP/PA\", axis=1)\n",
        "val_full = val_full.drop(\"AP/PA\", axis=1)\n",
        "train_full = train_full.fillna(0)\n",
        "val_full = val_full.fillna(0)\n",
        "\n",
        "train_dataset_full = CustomDataset(train_full)\n",
        "val_dataset_full = CustomDataset(val_full)\n",
        "\n",
        "train_loader_full = DataLoader(train_dataset_full, batch_size=16, shuffle=False)\n",
        "val_loader_full = DataLoader(val_dataset_full, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "q1RLzWP_qnef"
      },
      "id": "q1RLzWP_qnef",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models with best weights from initial training\n",
        "text_enc_full = TextEncoder(embed_dim=512).to(device)\n",
        "vision_enc_full = VisionEncoder(embed_dim=512).to(device)\n",
        "\n",
        "# Load the best weights\n",
        "best_path = \"/content/drive/MyDrive/models/vision_text_new/checkpoint_iter_25000.pt\"\n",
        "state_dict_text = torch.load(best_path)[\"text_enc\"]\n",
        "missing, unexpected = text_enc_full.load_state_dict(state_dict_text, strict=True)\n",
        "\n",
        "state_dict_vision = torch.load(best_path)[\"vision_enc\"]\n",
        "vision_enc_full.load_state_dict(state_dict_vision, strict=True)\n",
        "# Also option to do final weights from last checkpoint (end of 5 epochs)\n",
        "\n",
        "\n",
        "SAVE_PATH_FULL = \"/content/drive/MyDrive/models/vision_text_full_data/\"\n",
        "\n",
        "os.makedirs(SAVE_PATH_FULL, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzkQ79qIn1_v",
        "outputId": "66e494cc-ca2a-4334-cac2-043f9967493b"
      },
      "id": "OzkQ79qIn1_v",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt50_32X4D_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Setup\n",
        "# =========================\n",
        "temperature = 0.07\n",
        "max_epoch_number = 3\n",
        "test_freq = 1000            # validation frequency\n",
        "ckpt_freq = 1000            # checkpoint frequency (iterations)\n",
        "\n",
        "\n",
        "# Optimizer\n",
        "# Use even further differentiated learning rates this time\n",
        "proj_lr = 1e-3      # new layers, small + need to move fast\n",
        "vision_lr = 1e-4    # 1e-4 if you’re okay being a bit bolder\n",
        "bert_lr = 2e-5      # already in the usual BERT fine-tuning range\n",
        "\n",
        "proj_parameters = list(text_enc_full.proj.parameters()) + list(vision_enc_full.proj.parameters())\n",
        "\n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': text_enc_full.bert.parameters(), 'lr': bert_lr},\n",
        "    {'params': proj_parameters, 'lr': proj_lr},\n",
        "    {'params': vision_enc_full.backbone.parameters(), 'lr': vision_lr}\n",
        "])\n",
        "\n",
        "# Freeze BERT + ResNet weights\n",
        "# for p in text_enc.bert.parameters():\n",
        "#     p.requires_grad = False\n",
        "\n",
        "# for p in vision_enc.backbone.parameters():\n",
        "#     p.requires_grad = False\n",
        "\n",
        "# # Only projection layers train\n",
        "# proj_params = list(text_enc.proj.parameters()) + list(vision_enc.proj.parameters())\n",
        "# optimizer = torch.optim.Adam(proj_params, lr=1e-3)  # you can afford a higher LR here\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Mixed Precision Setup\n",
        "# =========================\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "iteration = 0\n",
        "for epoch in range(max_epoch_number):\n",
        "    text_enc_full.train()\n",
        "    vision_enc_full.train()\n",
        "    batch_losses = []\n",
        "\n",
        "    for batch in train_loader_full:\n",
        "        images = batch[\"image_tensor\"].to(device)\n",
        "        token_ids = batch[\"token_ids\"].squeeze(1).to(device)\n",
        "        attention_masks = batch[\"attention_masks\"].squeeze(1).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # =========================\n",
        "        # Forward with mixed precision\n",
        "        # =========================\n",
        "        with torch.cuda.amp.autocast():\n",
        "            image_embeddings = vision_enc_full(images)\n",
        "            text_embeddings = text_enc_full(token_ids, attention_masks)\n",
        "\n",
        "            if torch.isnan(image_embeddings).any():\n",
        "                print(\"NaN in image_embeddings\"); break\n",
        "            if torch.isnan(text_embeddings).any():\n",
        "                print(\"NaN in text_embeddings\"); break\n",
        "\n",
        "            loss = contrastive_loss(image_embeddings, text_embeddings, temperature)\n",
        "\n",
        "            if torch.isnan(loss).any():\n",
        "                print(\"NaN in loss\"); break\n",
        "\n",
        "        # =========================\n",
        "        # Backward with gradient scaling\n",
        "        # =========================\n",
        "        batch_loss_value = loss.item()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        batch_losses.append(batch_loss_value)\n",
        "\n",
        "        # =========================\n",
        "        # Validation\n",
        "        # =========================\n",
        "        if iteration % test_freq == 0:\n",
        "            text_enc_full.eval()\n",
        "            vision_enc_full.eval()\n",
        "            val_losses = []\n",
        "            all_img_embeds = []\n",
        "            all_text_embeds = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch_val in val_loader_full:\n",
        "                    val_images = batch_val[\"image_tensor\"].to(device)\n",
        "                    val_token_ids = batch_val[\"token_ids\"].squeeze(1).to(device)\n",
        "                    val_attention_masks = batch_val[\"attention_masks\"].squeeze(1).to(device)\n",
        "\n",
        "                    # Mixed precision for validation too\n",
        "                    with torch.cuda.amp.autocast():\n",
        "                        val_img_embeds = vision_enc_full(val_images)\n",
        "                        val_txt_embeds = text_enc_full(val_token_ids, val_attention_masks)\n",
        "                        val_loss = contrastive_loss(val_img_embeds, val_txt_embeds, temperature)\n",
        "\n",
        "                    val_losses.append(val_loss.item())\n",
        "                    all_img_embeds.append(val_img_embeds.cpu())\n",
        "                    all_text_embeds.append(val_txt_embeds.cpu())\n",
        "\n",
        "            all_img_embeds = torch.cat(all_img_embeds, dim=0)   # shape (N, d)\n",
        "            all_text_embeds = torch.cat(all_text_embeds, dim=0)\n",
        "\n",
        "            avg_val_loss = float(np.mean(val_losses))\n",
        "            recall_results = recall_at_k(all_img_embeds, all_text_embeds)\n",
        "            rounded_recall_results = {key: round(value, 5) for key, value in recall_results.items()}\n",
        "            print(f\"epoch:{epoch+1:2d} iter:{iteration:4d} val loss:{avg_val_loss:.3f}, recall@k: {rounded_recall_results}\")\n",
        "\n",
        "            text_enc_full.train()\n",
        "            vision_enc_full.train()\n",
        "\n",
        "        # =========================\n",
        "        # Periodic checkpoint\n",
        "        # =========================\n",
        "        if iteration % ckpt_freq == 0:\n",
        "            ckpt_path = os.path.join(SAVE_PATH_FULL, f\"checkpoint_iter_{iteration}.pt\")\n",
        "            torch.save(\n",
        "                {\n",
        "                    \"epoch\": epoch,\n",
        "                    \"iteration\": iteration,\n",
        "                    \"text_enc\": text_enc_full.state_dict(),\n",
        "                    \"vision_enc\": vision_enc_full.state_dict(),\n",
        "                    \"optimizer\": optimizer.state_dict(),\n",
        "                    \"scaler\": scaler.state_dict(),  # Save scaler state\n",
        "                    \"train_loss\": float(batch_loss_value),\n",
        "                },\n",
        "                ckpt_path,\n",
        "            )\n",
        "            print(f\"Checkpoint saved to {ckpt_path}\")\n",
        "\n",
        "        iteration += 1\n",
        "\n",
        "    train_loss = float(np.mean(batch_losses))\n",
        "    print(f\"epoch:{epoch+1:2d} iter:{iteration:4d} train loss:{train_loss:.3f}\\n\")\n",
        "\n",
        "# =========================\n",
        "# Final checkpoint\n",
        "# =========================\n",
        "final_path = \"/content/final_checkpoint_full_data.pt\"\n",
        "torch.save(\n",
        "    {\n",
        "        \"epoch\": epoch,\n",
        "        \"iteration\": iteration,\n",
        "        \"text_enc\": text_enc_full.state_dict(),\n",
        "        \"vision_enc\": vision_enc_full.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "        \"scaler\": scaler.state_dict(),  # Save scaler state\n",
        "        \"train_loss\": train_loss,\n",
        "    },\n",
        "    final_path,\n",
        ")\n",
        "print(f\"Final checkpoint saved to {final_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLv915awmC_Y",
        "outputId": "bb4a0233-d399-4c19-846a-14b9266c6f31"
      },
      "id": "fLv915awmC_Y",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2398437691.py:39: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "/tmp/ipython-input-2398437691.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "/tmp/ipython-input-2398437691.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 iter:   0 val loss:2.141, recall@k: {'image_to_text_recall@1': 0.04274, 'image_to_text_recall@5': 0.11538, 'text_to_image_recall@1': 0.05128, 'text_to_image_recall@5': 0.16239}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_0.pt\n",
            "epoch: 1 iter:1000 val loss:2.028, recall@k: {'image_to_text_recall@1': 0.05556, 'image_to_text_recall@5': 0.17094, 'text_to_image_recall@1': 0.04274, 'text_to_image_recall@5': 0.18803}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_1000.pt\n",
            "epoch: 1 iter:2000 val loss:1.890, recall@k: {'image_to_text_recall@1': 0.03846, 'image_to_text_recall@5': 0.20085, 'text_to_image_recall@1': 0.05128, 'text_to_image_recall@5': 0.21368}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_2000.pt\n",
            "epoch: 1 iter:3000 val loss:1.961, recall@k: {'image_to_text_recall@1': 0.05128, 'image_to_text_recall@5': 0.1453, 'text_to_image_recall@1': 0.05556, 'text_to_image_recall@5': 0.18376}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_3000.pt\n",
            "epoch: 1 iter:4000 val loss:1.965, recall@k: {'image_to_text_recall@1': 0.03846, 'image_to_text_recall@5': 0.17094, 'text_to_image_recall@1': 0.05128, 'text_to_image_recall@5': 0.18376}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_4000.pt\n",
            "epoch: 1 iter:5000 val loss:1.905, recall@k: {'image_to_text_recall@1': 0.04274, 'image_to_text_recall@5': 0.13675, 'text_to_image_recall@1': 0.03419, 'text_to_image_recall@5': 0.15385}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_5000.pt\n",
            "epoch: 1 iter:6000 val loss:1.972, recall@k: {'image_to_text_recall@1': 0.05983, 'image_to_text_recall@5': 0.1453, 'text_to_image_recall@1': 0.04701, 'text_to_image_recall@5': 0.17094}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_6000.pt\n",
            "epoch: 1 iter:7000 val loss:1.883, recall@k: {'image_to_text_recall@1': 0.05556, 'image_to_text_recall@5': 0.18803, 'text_to_image_recall@1': 0.05128, 'text_to_image_recall@5': 0.21795}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_7000.pt\n",
            "epoch: 1 iter:8000 val loss:1.899, recall@k: {'image_to_text_recall@1': 0.04701, 'image_to_text_recall@5': 0.20085, 'text_to_image_recall@1': 0.03846, 'text_to_image_recall@5': 0.17094}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_8000.pt\n",
            "epoch: 1 iter:9000 val loss:1.863, recall@k: {'image_to_text_recall@1': 0.0641, 'image_to_text_recall@5': 0.18803, 'text_to_image_recall@1': 0.04701, 'text_to_image_recall@5': 0.19231}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_9000.pt\n",
            "epoch: 1 iter:10000 val loss:2.078, recall@k: {'image_to_text_recall@1': 0.05128, 'image_to_text_recall@5': 0.15385, 'text_to_image_recall@1': 0.03846, 'text_to_image_recall@5': 0.17521}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_10000.pt\n",
            "epoch: 1 iter:11000 val loss:1.939, recall@k: {'image_to_text_recall@1': 0.05128, 'image_to_text_recall@5': 0.15812, 'text_to_image_recall@1': 0.04274, 'text_to_image_recall@5': 0.18376}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_11000.pt\n",
            "epoch: 1 iter:12000 val loss:1.893, recall@k: {'image_to_text_recall@1': 0.05556, 'image_to_text_recall@5': 0.17949, 'text_to_image_recall@1': 0.04701, 'text_to_image_recall@5': 0.20085}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_12000.pt\n",
            "epoch: 1 iter:13000 val loss:1.974, recall@k: {'image_to_text_recall@1': 0.03846, 'image_to_text_recall@5': 0.13248, 'text_to_image_recall@1': 0.04274, 'text_to_image_recall@5': 0.20513}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_13000.pt\n",
            "epoch: 1 iter:13964 train loss:2.197\n",
            "\n",
            "epoch: 2 iter:14000 val loss:2.004, recall@k: {'image_to_text_recall@1': 0.03419, 'image_to_text_recall@5': 0.15385, 'text_to_image_recall@1': 0.05128, 'text_to_image_recall@5': 0.14957}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_14000.pt\n",
            "epoch: 2 iter:15000 val loss:1.908, recall@k: {'image_to_text_recall@1': 0.05128, 'image_to_text_recall@5': 0.15812, 'text_to_image_recall@1': 0.03419, 'text_to_image_recall@5': 0.17521}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_15000.pt\n",
            "epoch: 2 iter:16000 val loss:1.945, recall@k: {'image_to_text_recall@1': 0.03846, 'image_to_text_recall@5': 0.15385, 'text_to_image_recall@1': 0.04701, 'text_to_image_recall@5': 0.17949}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_16000.pt\n",
            "epoch: 2 iter:17000 val loss:1.993, recall@k: {'image_to_text_recall@1': 0.04274, 'image_to_text_recall@5': 0.13248, 'text_to_image_recall@1': 0.05128, 'text_to_image_recall@5': 0.19231}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_17000.pt\n",
            "epoch: 2 iter:18000 val loss:1.944, recall@k: {'image_to_text_recall@1': 0.04274, 'image_to_text_recall@5': 0.12393, 'text_to_image_recall@1': 0.02991, 'text_to_image_recall@5': 0.17521}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_18000.pt\n",
            "epoch: 2 iter:19000 val loss:1.876, recall@k: {'image_to_text_recall@1': 0.04274, 'image_to_text_recall@5': 0.18803, 'text_to_image_recall@1': 0.05128, 'text_to_image_recall@5': 0.17521}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_19000.pt\n",
            "epoch: 2 iter:20000 val loss:1.942, recall@k: {'image_to_text_recall@1': 0.05556, 'image_to_text_recall@5': 0.13248, 'text_to_image_recall@1': 0.02564, 'text_to_image_recall@5': 0.18376}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_20000.pt\n",
            "epoch: 2 iter:21000 val loss:1.942, recall@k: {'image_to_text_recall@1': 0.05128, 'image_to_text_recall@5': 0.17094, 'text_to_image_recall@1': 0.05556, 'text_to_image_recall@5': 0.17094}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_21000.pt\n",
            "epoch: 2 iter:22000 val loss:1.972, recall@k: {'image_to_text_recall@1': 0.04274, 'image_to_text_recall@5': 0.11966, 'text_to_image_recall@1': 0.05128, 'text_to_image_recall@5': 0.17094}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_22000.pt\n",
            "epoch: 2 iter:23000 val loss:2.092, recall@k: {'image_to_text_recall@1': 0.03846, 'image_to_text_recall@5': 0.10684, 'text_to_image_recall@1': 0.03846, 'text_to_image_recall@5': 0.14957}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_23000.pt\n",
            "epoch: 2 iter:24000 val loss:2.118, recall@k: {'image_to_text_recall@1': 0.05128, 'image_to_text_recall@5': 0.1453, 'text_to_image_recall@1': 0.05983, 'text_to_image_recall@5': 0.20085}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_24000.pt\n",
            "epoch: 2 iter:25000 val loss:2.008, recall@k: {'image_to_text_recall@1': 0.03846, 'image_to_text_recall@5': 0.1453, 'text_to_image_recall@1': 0.03846, 'text_to_image_recall@5': 0.16239}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_25000.pt\n",
            "epoch: 2 iter:26000 val loss:1.920, recall@k: {'image_to_text_recall@1': 0.03846, 'image_to_text_recall@5': 0.14103, 'text_to_image_recall@1': 0.04701, 'text_to_image_recall@5': 0.13675}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_26000.pt\n",
            "epoch: 2 iter:27000 val loss:1.888, recall@k: {'image_to_text_recall@1': 0.04274, 'image_to_text_recall@5': 0.15812, 'text_to_image_recall@1': 0.05556, 'text_to_image_recall@5': 0.15812}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_27000.pt\n",
            "epoch: 2 iter:27928 train loss:2.055\n",
            "\n",
            "epoch: 3 iter:28000 val loss:2.067, recall@k: {'image_to_text_recall@1': 0.03419, 'image_to_text_recall@5': 0.13675, 'text_to_image_recall@1': 0.03846, 'text_to_image_recall@5': 0.1453}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_28000.pt\n",
            "epoch: 3 iter:29000 val loss:1.938, recall@k: {'image_to_text_recall@1': 0.03846, 'image_to_text_recall@5': 0.16667, 'text_to_image_recall@1': 0.05983, 'text_to_image_recall@5': 0.16667}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_29000.pt\n",
            "epoch: 3 iter:30000 val loss:1.984, recall@k: {'image_to_text_recall@1': 0.02991, 'image_to_text_recall@5': 0.15385, 'text_to_image_recall@1': 0.04701, 'text_to_image_recall@5': 0.17949}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_30000.pt\n",
            "epoch: 3 iter:31000 val loss:2.115, recall@k: {'image_to_text_recall@1': 0.02564, 'image_to_text_recall@5': 0.12393, 'text_to_image_recall@1': 0.03846, 'text_to_image_recall@5': 0.15812}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_31000.pt\n",
            "epoch: 3 iter:32000 val loss:2.013, recall@k: {'image_to_text_recall@1': 0.04274, 'image_to_text_recall@5': 0.12393, 'text_to_image_recall@1': 0.05556, 'text_to_image_recall@5': 0.16667}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_32000.pt\n",
            "epoch: 3 iter:33000 val loss:2.004, recall@k: {'image_to_text_recall@1': 0.03419, 'image_to_text_recall@5': 0.14103, 'text_to_image_recall@1': 0.03846, 'text_to_image_recall@5': 0.14957}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_33000.pt\n",
            "epoch: 3 iter:34000 val loss:2.070, recall@k: {'image_to_text_recall@1': 0.04701, 'image_to_text_recall@5': 0.11538, 'text_to_image_recall@1': 0.03846, 'text_to_image_recall@5': 0.16667}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_34000.pt\n",
            "epoch: 3 iter:35000 val loss:2.002, recall@k: {'image_to_text_recall@1': 0.04701, 'image_to_text_recall@5': 0.17949, 'text_to_image_recall@1': 0.05556, 'text_to_image_recall@5': 0.16239}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_35000.pt\n",
            "epoch: 3 iter:36000 val loss:2.028, recall@k: {'image_to_text_recall@1': 0.03846, 'image_to_text_recall@5': 0.14103, 'text_to_image_recall@1': 0.04701, 'text_to_image_recall@5': 0.17949}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_36000.pt\n",
            "epoch: 3 iter:37000 val loss:2.139, recall@k: {'image_to_text_recall@1': 0.02991, 'image_to_text_recall@5': 0.1453, 'text_to_image_recall@1': 0.02991, 'text_to_image_recall@5': 0.19231}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_37000.pt\n",
            "epoch: 3 iter:38000 val loss:2.114, recall@k: {'image_to_text_recall@1': 0.03419, 'image_to_text_recall@5': 0.10684, 'text_to_image_recall@1': 0.04701, 'text_to_image_recall@5': 0.15812}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_38000.pt\n",
            "epoch: 3 iter:39000 val loss:2.080, recall@k: {'image_to_text_recall@1': 0.02564, 'image_to_text_recall@5': 0.14957, 'text_to_image_recall@1': 0.02991, 'text_to_image_recall@5': 0.15812}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_39000.pt\n",
            "epoch: 3 iter:40000 val loss:2.085, recall@k: {'image_to_text_recall@1': 0.04701, 'image_to_text_recall@5': 0.16239, 'text_to_image_recall@1': 0.03419, 'text_to_image_recall@5': 0.15812}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_40000.pt\n",
            "epoch: 3 iter:41000 val loss:2.041, recall@k: {'image_to_text_recall@1': 0.04274, 'image_to_text_recall@5': 0.15385, 'text_to_image_recall@1': 0.05128, 'text_to_image_recall@5': 0.15385}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/checkpoint_iter_41000.pt\n",
            "epoch: 3 iter:41892 train loss:1.846\n",
            "\n",
            "Final checkpoint saved to /content/final_checkpoint_full_data.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_path_f = os.path.join(SAVE_PATH_FULL, \"final_checkpoint.pt\")\n",
        "\n",
        "torch.save(\n",
        "    {\n",
        "        \"epoch\": epoch,\n",
        "        \"iteration\": iteration,\n",
        "        \"text_enc\": text_enc_full.state_dict(),\n",
        "        \"vision_enc\": vision_enc_full.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "        \"scaler\": scaler.state_dict(),  # Save scaler state\n",
        "        \"train_loss\": train_loss,\n",
        "    },\n",
        "    final_path_f,\n",
        ")\n",
        "print(f\"Final checkpoint saved to {final_path_f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHPHymb8weuI",
        "outputId": "3ecf3218-f31b-482a-e227-45fdbe576bf5"
      },
      "id": "LHPHymb8weuI",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final checkpoint saved to /content/drive/MyDrive/models/vision_text_full_data/final_checkpoint.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try with Clinical Bert as Text Encoder Backbone"
      ],
      "metadata": {
        "id": "kK3Q-rEYqRZJ"
      },
      "id": "kK3Q-rEYqRZJ"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "SAVE_PATH = \"/content/drive/MyDrive/models/vision_text_clinical_full/\"\n",
        "\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUMlZCunAbin",
        "outputId": "7f3cc991-88df-4e0f-f924-8f22ad773231"
      },
      "id": "gUMlZCunAbin",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_report_updated(row):\n",
        "    labels = row.iloc[5:]  # Skip Path, Sex, Age, Frontal/Lateral, AP/PA\n",
        "\n",
        "    # Separate findings by certainty\n",
        "    positive_findings = list(labels[labels == 1.0].index)\n",
        "    uncertain_findings = list(labels[labels == -1.0].index)\n",
        "\n",
        "    # Build the report\n",
        "    report_parts = []\n",
        "\n",
        "    # Add patient demographics for context\n",
        "    age = int(row['Age']) if pd.notna(row['Age']) else None\n",
        "    sex = row['Sex'].lower() if pd.notna(row['Sex']) else None\n",
        "    view = row['Frontal/Lateral'].lower() if pd.notna(row['Frontal/Lateral']) else None\n",
        "\n",
        "    # Start with view type\n",
        "    if view:\n",
        "        report_parts.append(f\"{view.capitalize()} chest radiograph\")\n",
        "    else:\n",
        "        report_parts.append(\"Chest radiograph\")\n",
        "\n",
        "    # Add demographics\n",
        "    demo = []\n",
        "    if age:\n",
        "        demo.append(f\"{age}-year-old\")\n",
        "    if sex:\n",
        "        demo.append(sex)\n",
        "    if demo:\n",
        "        report_parts.append(f\"of {' '.join(demo)} patient\")\n",
        "\n",
        "    # Add findings\n",
        "    if len(positive_findings) == 0 and len(uncertain_findings) == 0:\n",
        "        report_parts.append(\"demonstrates no acute cardiopulmonary abnormality\")\n",
        "    else:\n",
        "        findings_text = []\n",
        "\n",
        "        # Definite findings\n",
        "        if positive_findings:\n",
        "            findings_clean = [f.lower().replace('_', ' ') for f in positive_findings]\n",
        "            findings_text.append(\"shows \" + \", \".join(findings_clean))\n",
        "\n",
        "        # Uncertain findings (optional - you might want to treat these differently)\n",
        "        if uncertain_findings:\n",
        "            uncertain_clean = [f.lower().replace('_', ' ') for f in uncertain_findings]\n",
        "            findings_text.append(\"possible \" + \", \".join(uncertain_clean))\n",
        "\n",
        "        report_parts.append(\". \".join(findings_text))\n",
        "\n",
        "    return \" \".join(report_parts) + \".\"\n"
      ],
      "metadata": {
        "id": "ndVcxYWciuap"
      },
      "id": "ndVcxYWciuap",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "MODEL_NAME = \"emilyalsentzer/Bio_ClinicalBERT\""
      ],
      "metadata": {
        "id": "B5wgUKOpwSYI"
      },
      "id": "B5wgUKOpwSYI",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClinicalTextEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim=512):\n",
        "        super(ClinicalTextEncoder, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained(\n",
        "            MODEL_NAME\n",
        "        )\n",
        "        self.proj = nn.Linear(768, embed_dim)\n",
        "\n",
        "    def forward(self, token_ids, attention_masks):\n",
        "        outputs = self.bert(token_ids, attention_mask=attention_masks)\n",
        "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "        embeddings = self.proj(cls_embeddings)\n",
        "        # normalizing because we need to compare with image embeddings later\n",
        "        # for the contrastive similarity\n",
        "        embeddings = F.normalize(embeddings, p=2, dim=-1, eps=1e-6)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "njsSCNxtqTqQ"
      },
      "id": "njsSCNxtqTqQ",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ClinicalCustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df):\n",
        "        df = df.reset_index(drop=True) # Reset index to ensure 0-based indexing\n",
        "        # Text\n",
        "        self.text_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "        self.reports = df.apply(\n",
        "            generate_report_updated, axis=1\n",
        "        )  # Maybe move generate report to inside the dataset later\n",
        "\n",
        "        # Vision\n",
        "        self.images = df[\"Path\"]\n",
        "        self.transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize((256, 256)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reports)  # This could work or we could do another way\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Text part\n",
        "        report = self.reports[idx]\n",
        "        encoder = self.text_tokenizer.encode_plus(\n",
        "            report,\n",
        "            max_length=512,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        # Vision part\n",
        "        img_path = self.images[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        img_tensor = self.transform(image)\n",
        "\n",
        "        return {\n",
        "            \"token_ids\": encoder[\"input_ids\"],\n",
        "            \"attention_masks\": encoder[\"attention_mask\"],\n",
        "            \"image_tensor\": img_tensor,\n",
        "        }  # Is vision part correct?"
      ],
      "metadata": {
        "id": "qYz6zYYXqf4N"
      },
      "id": "qYz6zYYXqf4N",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_clinical = ClinicalCustomDataset(train_subset)\n",
        "val_dataset_clinical = ClinicalCustomDataset(val_full) # Use full validation data since it's not that big anyway\n",
        "\n",
        "train_loader_clinical = DataLoader(train_dataset_clinical, batch_size=32, shuffle=True)\n",
        "val_loader_clinical = DataLoader(val_dataset_clinical, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "wVAK0_oMqozG"
      },
      "id": "wVAK0_oMqozG",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_PATH_CLINICAL = \"/content/drive/MyDrive/models/vision_text_clinical/\"\n",
        "\n",
        "os.makedirs(SAVE_PATH_CLINICAL, exist_ok=True)"
      ],
      "metadata": {
        "id": "_bMve1iss4ON"
      },
      "id": "_bMve1iss4ON",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Setup\n",
        "# =========================\n",
        "temperature = 0.2\n",
        "max_epoch_number = 3\n",
        "test_freq = 1000            # validation frequency\n",
        "ckpt_freq = 1000            # checkpoint frequency (iterations)\n",
        "\n",
        "# Initialize models\n",
        "text_enc = ClinicalTextEncoder(embed_dim=512).to(device)\n",
        "vision_enc = VisionEncoder(embed_dim=512).to(device)\n",
        "\n",
        "# Optimizer\n",
        "# Use even further differentiated learning rates this time\n",
        "proj_lr = 1e-3      # new layers, small + need to move fast\n",
        "vision_lr = 5e-5    # 1e-4 if you’re okay being a bit bolder\n",
        "bert_lr = 3e-5      # already in the usual BERT fine-tuning range\n",
        "\n",
        "proj_parameters = list(text_enc.proj.parameters()) + list(vision_enc.proj.parameters())\n",
        "\n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': text_enc.bert.parameters(), 'lr': bert_lr},\n",
        "    {'params': proj_parameters, 'lr': proj_lr},\n",
        "    {'params': vision_enc.backbone.parameters(), 'lr': vision_lr}\n",
        "])\n",
        "\n",
        "# =========================\n",
        "# Mixed Precision Setup\n",
        "# =========================\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "iteration = 0\n",
        "for epoch in range(max_epoch_number):\n",
        "    text_enc.train()\n",
        "    vision_enc.train()\n",
        "    batch_losses = []\n",
        "\n",
        "    for batch in train_loader_clinical:\n",
        "        images = batch[\"image_tensor\"].to(device)\n",
        "        token_ids = batch[\"token_ids\"].squeeze(1).to(device)\n",
        "        attention_masks = batch[\"attention_masks\"].squeeze(1).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # =========================\n",
        "        # Forward with mixed precision\n",
        "        # =========================\n",
        "        with torch.cuda.amp.autocast():\n",
        "            image_embeddings = vision_enc(images)\n",
        "            text_embeddings = text_enc(token_ids, attention_masks)\n",
        "\n",
        "            if torch.isnan(image_embeddings).any():\n",
        "                print(\"NaN in image_embeddings\"); break\n",
        "            if torch.isnan(text_embeddings).any():\n",
        "                print(\"NaN in text_embeddings\"); break\n",
        "\n",
        "            loss = contrastive_loss(image_embeddings, text_embeddings, temperature)\n",
        "\n",
        "            if torch.isnan(loss).any():\n",
        "                print(\"NaN in loss\"); break\n",
        "\n",
        "        # =========================\n",
        "        # Backward with gradient scaling\n",
        "        # =========================\n",
        "        batch_loss_value = loss.item()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        batch_losses.append(batch_loss_value)\n",
        "\n",
        "        # =========================\n",
        "        # Validation\n",
        "        # =========================\n",
        "        if iteration % test_freq == 0:\n",
        "            text_enc.eval()\n",
        "            vision_enc.eval()\n",
        "            val_losses = []\n",
        "            all_img_embeds = []\n",
        "            all_text_embeds = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch_val in val_loader_clinical:\n",
        "                    val_images = batch_val[\"image_tensor\"].to(device)\n",
        "                    val_token_ids = batch_val[\"token_ids\"].squeeze(1).to(device)\n",
        "                    val_attention_masks = batch_val[\"attention_masks\"].squeeze(1).to(device)\n",
        "\n",
        "                    # Mixed precision for validation too\n",
        "                    with torch.cuda.amp.autocast():\n",
        "                        val_img_embeds = vision_enc(val_images)\n",
        "                        val_txt_embeds = text_enc(val_token_ids, val_attention_masks)\n",
        "                        val_loss = contrastive_loss(val_img_embeds, val_txt_embeds, temperature)\n",
        "\n",
        "                    val_losses.append(val_loss.item())\n",
        "                    all_img_embeds.append(val_img_embeds.cpu())\n",
        "                    all_text_embeds.append(val_txt_embeds.cpu())\n",
        "\n",
        "            all_img_embeds = torch.cat(all_img_embeds, dim=0)   # shape (N, d)\n",
        "            all_text_embeds = torch.cat(all_text_embeds, dim=0)\n",
        "\n",
        "            avg_val_loss = float(np.mean(val_losses))\n",
        "            recall_results = recall_at_k(all_img_embeds, all_text_embeds)\n",
        "            rounded_recall_results = {key: round(value, 5) for key, value in recall_results.items()}\n",
        "            print(f\"epoch:{epoch+1:2d} iter:{iteration:4d} val loss:{avg_val_loss:.3f}, recall@k: {rounded_recall_results}\")\n",
        "\n",
        "            text_enc.train()\n",
        "            vision_enc.train()\n",
        "\n",
        "        # =========================\n",
        "        # Periodic checkpoint\n",
        "        # =========================\n",
        "        if iteration % ckpt_freq == 0:\n",
        "            ckpt_path = os.path.join(SAVE_PATH_CLINICAL, f\"checkpoint_iter_{iteration}.pt\")\n",
        "            torch.save(\n",
        "                {\n",
        "                    \"epoch\": epoch,\n",
        "                    \"iteration\": iteration,\n",
        "                    \"text_enc\": text_enc.state_dict(),\n",
        "                    \"vision_enc\": vision_enc.state_dict(),\n",
        "                    \"optimizer\": optimizer.state_dict(),\n",
        "                    \"scaler\": scaler.state_dict(),  # Save scaler state\n",
        "                    \"train_loss\": float(batch_loss_value),\n",
        "                },\n",
        "                ckpt_path,\n",
        "            )\n",
        "            print(f\"Checkpoint saved to {ckpt_path}\")\n",
        "\n",
        "        iteration += 1\n",
        "\n",
        "    train_loss = float(np.mean(batch_losses))\n",
        "    print(f\"epoch:{epoch+1:2d} iter:{iteration:4d} train loss:{train_loss:.3f}\\n\")\n",
        "\n",
        "# =========================\n",
        "# Final checkpoint\n",
        "# =========================\n",
        "final_path = os.path.join(SAVE_PATH_CLINICAL, \"final_checkpoint_clinical.pt\")\n",
        "torch.save(\n",
        "    {\n",
        "        \"epoch\": epoch,\n",
        "        \"iteration\": iteration,\n",
        "        \"text_enc\": text_enc.state_dict(),\n",
        "        \"vision_enc\": vision_enc.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "        \"scaler\": scaler.state_dict(),  # Save scaler state\n",
        "        \"train_loss\": train_loss,\n",
        "    },\n",
        "    final_path,\n",
        ")\n",
        "print(f\"Final checkpoint saved to {final_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZBxhDtSq5UW",
        "outputId": "8ac2687d-6d00-4e10-8428-56819fa75eba"
      },
      "id": "fZBxhDtSq5UW",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt50_32X4D_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/tmp/ipython-input-3692247217.py:30: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "/tmp/ipython-input-3692247217.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "/tmp/ipython-input-3692247217.py:89: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 iter:   0 val loss:3.313, recall@k: {'image_to_text_recall@1': 0.00427, 'image_to_text_recall@5': 0.02991, 'text_to_image_recall@1': 0.00427, 'text_to_image_recall@5': 0.03419}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_clinical/checkpoint_iter_0.pt\n",
            "epoch: 1 iter:1000 val loss:1.707, recall@k: {'image_to_text_recall@1': 0.12821, 'image_to_text_recall@5': 0.4188, 'text_to_image_recall@1': 0.08547, 'text_to_image_recall@5': 0.39744}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_clinical/checkpoint_iter_1000.pt\n",
            "epoch: 1 iter:2000 val loss:1.528, recall@k: {'image_to_text_recall@1': 0.15385, 'image_to_text_recall@5': 0.50427, 'text_to_image_recall@1': 0.15385, 'text_to_image_recall@5': 0.48718}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_clinical/checkpoint_iter_2000.pt\n",
            "epoch: 1 iter:3000 val loss:1.490, recall@k: {'image_to_text_recall@1': 0.17521, 'image_to_text_recall@5': 0.49573, 'text_to_image_recall@1': 0.1453, 'text_to_image_recall@5': 0.48718}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_clinical/checkpoint_iter_3000.pt\n",
            "epoch: 1 iter:3491 train loss:1.730\n",
            "\n",
            "epoch: 2 iter:4000 val loss:1.472, recall@k: {'image_to_text_recall@1': 0.19658, 'image_to_text_recall@5': 0.57265, 'text_to_image_recall@1': 0.14103, 'text_to_image_recall@5': 0.57265}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_clinical/checkpoint_iter_4000.pt\n",
            "epoch: 2 iter:5000 val loss:1.420, recall@k: {'image_to_text_recall@1': 0.18803, 'image_to_text_recall@5': 0.54701, 'text_to_image_recall@1': 0.19658, 'text_to_image_recall@5': 0.52991}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_clinical/checkpoint_iter_5000.pt\n",
            "epoch: 2 iter:6000 val loss:1.440, recall@k: {'image_to_text_recall@1': 0.18803, 'image_to_text_recall@5': 0.53846, 'text_to_image_recall@1': 0.19658, 'text_to_image_recall@5': 0.55556}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_clinical/checkpoint_iter_6000.pt\n",
            "epoch: 2 iter:6982 train loss:1.400\n",
            "\n",
            "epoch: 3 iter:7000 val loss:1.379, recall@k: {'image_to_text_recall@1': 0.18376, 'image_to_text_recall@5': 0.5812, 'text_to_image_recall@1': 0.19231, 'text_to_image_recall@5': 0.55983}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_clinical/checkpoint_iter_7000.pt\n",
            "epoch: 3 iter:8000 val loss:1.443, recall@k: {'image_to_text_recall@1': 0.18376, 'image_to_text_recall@5': 0.5641, 'text_to_image_recall@1': 0.19658, 'text_to_image_recall@5': 0.52991}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_clinical/checkpoint_iter_8000.pt\n",
            "epoch: 3 iter:9000 val loss:1.374, recall@k: {'image_to_text_recall@1': 0.18803, 'image_to_text_recall@5': 0.55983, 'text_to_image_recall@1': 0.21368, 'text_to_image_recall@5': 0.56838}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_clinical/checkpoint_iter_9000.pt\n",
            "epoch: 3 iter:10000 val loss:1.388, recall@k: {'image_to_text_recall@1': 0.14957, 'image_to_text_recall@5': 0.58974, 'text_to_image_recall@1': 0.20513, 'text_to_image_recall@5': 0.53419}\n",
            "Checkpoint saved to /content/drive/MyDrive/models/vision_text_clinical/checkpoint_iter_10000.pt\n",
            "epoch: 3 iter:10473 train loss:1.246\n",
            "\n",
            "Final checkpoint saved to /content/drive/MyDrive/models/vision_text_clinical/final_checkpoint_clinical.pt\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a250e3a658694acabee3754dbcdce981": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4b0576d077eb4535a7458489e3ab2338",
              "IPY_MODEL_61ad6014b38f4950abaddc2c8048aea4",
              "IPY_MODEL_7ff0c2b01fba44aca8563b9b2d86d8c0"
            ],
            "layout": "IPY_MODEL_f4f23c759ad7442c9bcd31ab77794a7a"
          }
        },
        "4b0576d077eb4535a7458489e3ab2338": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6b9480588474f9a92ed7977997f2058",
            "placeholder": "​",
            "style": "IPY_MODEL_772be23f459b4fa2b8629acea64630bb",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "61ad6014b38f4950abaddc2c8048aea4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed623b0d22ac49239c1e710602ce3caf",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_df6346050a8a4c36ae4a952595997787",
            "value": 48
          }
        },
        "7ff0c2b01fba44aca8563b9b2d86d8c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3dd3c22733704be9a8d2c7915d7362d2",
            "placeholder": "​",
            "style": "IPY_MODEL_b8227d1aaddb42e7aaca4e46e2ef1feb",
            "value": " 48.0/48.0 [00:00&lt;00:00, 5.34kB/s]"
          }
        },
        "f4f23c759ad7442c9bcd31ab77794a7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6b9480588474f9a92ed7977997f2058": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "772be23f459b4fa2b8629acea64630bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed623b0d22ac49239c1e710602ce3caf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df6346050a8a4c36ae4a952595997787": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3dd3c22733704be9a8d2c7915d7362d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8227d1aaddb42e7aaca4e46e2ef1feb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85d28fa4f8b04f2ebc7c0e25b7ae9849": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0574df8bceb94f80b069013dc5933540",
              "IPY_MODEL_e9d473dd4bcb4ceea747edf2d79c1c1f",
              "IPY_MODEL_ec74d49d36e842f5a035540632c6c551"
            ],
            "layout": "IPY_MODEL_b9cee010b25c46d1ad25c0bbe845e009"
          }
        },
        "0574df8bceb94f80b069013dc5933540": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9eb0ab9585114fbb984bb12e5c74cb12",
            "placeholder": "​",
            "style": "IPY_MODEL_4ec2fa8d1bda4e1894fa9b732139b157",
            "value": "vocab.txt: 100%"
          }
        },
        "e9d473dd4bcb4ceea747edf2d79c1c1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0256cc8a00a64dad8930c50834c6cf68",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4aa54350339c46d88bdd15b6f2a751e6",
            "value": 231508
          }
        },
        "ec74d49d36e842f5a035540632c6c551": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_280ae84c88a0426e94efcdbcd3a6d2af",
            "placeholder": "​",
            "style": "IPY_MODEL_6ec1ed7796b04d4d99cf068d3e86d379",
            "value": " 232k/232k [00:00&lt;00:00, 651kB/s]"
          }
        },
        "b9cee010b25c46d1ad25c0bbe845e009": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9eb0ab9585114fbb984bb12e5c74cb12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ec2fa8d1bda4e1894fa9b732139b157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0256cc8a00a64dad8930c50834c6cf68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4aa54350339c46d88bdd15b6f2a751e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "280ae84c88a0426e94efcdbcd3a6d2af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ec1ed7796b04d4d99cf068d3e86d379": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ab5a1e1caa54eb3a43a436246dedd73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_838803d285a149ae85c0d0446d0a4707",
              "IPY_MODEL_95f6d7e5dde1484ca91305ce0cae343d",
              "IPY_MODEL_84ac86a997d7473e8dde41fd12a85ad0"
            ],
            "layout": "IPY_MODEL_84795388befa40968fe6b879ab6313ae"
          }
        },
        "838803d285a149ae85c0d0446d0a4707": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_215529a8aaad45bfb9fc834ad0cb5602",
            "placeholder": "​",
            "style": "IPY_MODEL_0a45c9f035854c5a938dfa784b130dc9",
            "value": "tokenizer.json: 100%"
          }
        },
        "95f6d7e5dde1484ca91305ce0cae343d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c77b74d62a04e9bbb998bbe52124d53",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_53b39fa0200748be872b735e0f22f494",
            "value": 466062
          }
        },
        "84ac86a997d7473e8dde41fd12a85ad0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9de1c5b59f94520a7cbe16f55c11f70",
            "placeholder": "​",
            "style": "IPY_MODEL_b0b9da5d7e0644f696da7563a8257206",
            "value": " 466k/466k [00:00&lt;00:00, 37.3MB/s]"
          }
        },
        "84795388befa40968fe6b879ab6313ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "215529a8aaad45bfb9fc834ad0cb5602": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a45c9f035854c5a938dfa784b130dc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c77b74d62a04e9bbb998bbe52124d53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53b39fa0200748be872b735e0f22f494": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f9de1c5b59f94520a7cbe16f55c11f70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0b9da5d7e0644f696da7563a8257206": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "606a15c383ca473cb3b8bdde7ec165d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_184ab117c899407f804b1c7e5413e8c0",
              "IPY_MODEL_38648b003c1d47ad8e5acc806385fb41",
              "IPY_MODEL_dff8aeb957ac4d0398db8c80cce2cb91"
            ],
            "layout": "IPY_MODEL_1927734c112242778f39340d1836b28c"
          }
        },
        "184ab117c899407f804b1c7e5413e8c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e304d09d7ee84816be0bf9e9d288c6cf",
            "placeholder": "​",
            "style": "IPY_MODEL_b9f47a76ce9743e9ad9c2f68fb191799",
            "value": "config.json: 100%"
          }
        },
        "38648b003c1d47ad8e5acc806385fb41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d54c8ea278941718d76f7381e2ba185",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_99222e948ea34487aac84be4637b7f06",
            "value": 570
          }
        },
        "dff8aeb957ac4d0398db8c80cce2cb91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c846b039610b430e8d93ab640bb5514f",
            "placeholder": "​",
            "style": "IPY_MODEL_f4b58bed9a394631963cd34defe06774",
            "value": " 570/570 [00:00&lt;00:00, 77.5kB/s]"
          }
        },
        "1927734c112242778f39340d1836b28c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e304d09d7ee84816be0bf9e9d288c6cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9f47a76ce9743e9ad9c2f68fb191799": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d54c8ea278941718d76f7381e2ba185": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99222e948ea34487aac84be4637b7f06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c846b039610b430e8d93ab640bb5514f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4b58bed9a394631963cd34defe06774": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "472f62dc31874c328a8a3526408063b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d1ec631f8ff14da0b980920a72c190f1",
              "IPY_MODEL_890f529f896c4208af7b57a2169efc6b",
              "IPY_MODEL_4ed491961fd948d6bb89e12334a26606"
            ],
            "layout": "IPY_MODEL_2ea20d3f74bd48c399b0d64196af3cbd"
          }
        },
        "d1ec631f8ff14da0b980920a72c190f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0fd2cda66da440fac413f2ed6b6b04c",
            "placeholder": "​",
            "style": "IPY_MODEL_9c758db06e3b4b1ca3cf75f29a14d95a",
            "value": "model.safetensors: 100%"
          }
        },
        "890f529f896c4208af7b57a2169efc6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ff9c897f2744c2cacbc6c62e999c163",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3096054248a4ca0b2e58c37aa901b84",
            "value": 440449768
          }
        },
        "4ed491961fd948d6bb89e12334a26606": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f4f344ef47f446cbcc7e9d7bff9e88f",
            "placeholder": "​",
            "style": "IPY_MODEL_12c4f2dfeaa14a6e8e7a61c658bccd3c",
            "value": " 440M/440M [00:05&lt;00:00, 135MB/s]"
          }
        },
        "2ea20d3f74bd48c399b0d64196af3cbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0fd2cda66da440fac413f2ed6b6b04c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c758db06e3b4b1ca3cf75f29a14d95a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ff9c897f2744c2cacbc6c62e999c163": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3096054248a4ca0b2e58c37aa901b84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f4f344ef47f446cbcc7e9d7bff9e88f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12c4f2dfeaa14a6e8e7a61c658bccd3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}