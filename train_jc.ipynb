{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8947a6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "#from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torchvision.models as models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a9aadc",
   "metadata": {},
   "source": [
    "### potential start for custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a1d305",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CHEX_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_names, labels):\n",
    "        self.image_names = image_names\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name, label = self.image_names[idx], self.labels[idx]\n",
    "        image_path = '/chexpert_dataset/train' + image_name\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5e12691",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_image_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# from homework but try to get the file names for every image\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_jpgs \u001b[38;5;241m=\u001b[39m \u001b[43mget_image_name\u001b[49m(train_set)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_image_name' is not defined"
     ]
    }
   ],
   "source": [
    "# from homework but try to get the file names for every image\n",
    "train_jpgs = get_image_name(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ddb6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to fix and make classes correct\n",
    "train_dataset = CHEX_Dataset(train_jpgs)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bea2f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1d1f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the torchvision's implementation of ResNeXt, but add FC layer for a different number of classes (27) and a Sigmoid instead of a default Softmax.\n",
    "class Resnext50(nn.Module):\n",
    "    def __init__(self, embed_dim=512):\n",
    "        super().__init__()\n",
    "        resnet = models.resnext50_32x4d(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        in_dim = resnet.fc.in_features\n",
    "        self.proj = nn.Linear(in_dim, embed_dim)\n",
    " \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        features=features.squeeze(-1).squeeze(-1)\n",
    "        z = self.proj(features)\n",
    "        # convert to unit vectors for cosine similarity later\n",
    "        z = z / z.norm(dim=-1, keepdim=True)\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b9f209",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Initialize the model\n",
    "image_encoder = Resnext50()\n",
    "# Switch model to the training mode\n",
    "image_encoder.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eac0ec",
   "metadata": {},
   "source": [
    "### Contrastive Loss\n",
    "-- not sure but just psuedocode from paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb28745c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_encoder - ResNet or Vision Transformer\n",
    "# text_encoder - CBOW or Text Transformer\n",
    "# I[n, h, w, c] - minibatch of aligned images\n",
    "# T[n, l] - minibatch of aligned texts\n",
    "# W_i[d_i, d_e] - learned proj of image to embed\n",
    "# W_t[d_t, d_e] - learned proj of text to embed\n",
    "# t - learned temperature parameter\n",
    "# extract feature representations of each modality\n",
    "I_f = image_encoder(I) #[n, d_i]\n",
    "T_f = text_encoder(T) #[n, d_t]\n",
    "# joint multimodal embedding [n, d_e]\n",
    "I_e = l2_normalize(np.dot(I_f, W_i), axis=1)\n",
    "T_e = l2_normalize(np.dot(T_f, W_t), axis=1)\n",
    "# scaled pairwise cosine similarities [n, n]\n",
    "logits = np.dot(I_e, T_e.T) * np.exp(t)\n",
    "# symmetric loss function\n",
    "labels = np.arange(n)\n",
    "loss_i = cross_entropy_loss(logits, labels, axis=0)\n",
    "loss_t = cross_entropy_loss(logits, labels, axis=1)\n",
    "loss = (loss_i + loss_t)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6ef059",
   "metadata": {},
   "source": [
    "### GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41c3b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCondGenerator(nn.Module):\n",
    "    def __init__(self, z_dim=128, t_dim=256, img_size=256):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(z_dim + t_dim, 512 * 4 * 4)\n",
    "        # then deconv / upsampling blocks to get [1,H,W]\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 1, 4, 2, 1),\n",
    "            nn.Tanh()     # outputs in [-1,1]\n",
    "        )\n",
    "\n",
    "    def forward(self, z_noise, z_text):\n",
    "        z = torch.cat([z_noise, z_text], dim=-1)\n",
    "        x = self.fc(z)\n",
    "        x = x.view(x.size(0), 512, 4, 4)\n",
    "        img = self.main(x)\n",
    "        return img\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
