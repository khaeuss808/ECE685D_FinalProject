{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8947a6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "#from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torchvision.models as models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a1d305",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CHEX_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_names, labels):\n",
    "        self.image_names = image_names\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name, label = self.image_names[idx], self.labels[idx]\n",
    "        image_path = '/chexpert_dataset/train' + image_name\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e12691",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_jpgs = get_image_name(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ddb6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VOC_Dataset(train_jpgs, multi_hot)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bea2f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1d1f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the torchvision's implementation of ResNeXt, but add FC layer for a different number of classes (27) and a Sigmoid instead of a default Softmax.\n",
    "class Resnext50(nn.Module):\n",
    "    def __init__(self, embed_dim=512):\n",
    "        super().__init__()\n",
    "        resnet = models.resnext50_32x4d(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        in_dim = resnet.fc.in_features\n",
    "        self.proj = nn.Linear(in_dim, embed_dim)\n",
    " \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        features=features.squeeze(-1).squeeze(-1)\n",
    "        z = self.proj(features)\n",
    "        # convert to unit vectors for cosine similarity later\n",
    "        z = z / z.norm(dim=-1, keepdim=True)\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b9f209",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Initialize the model\n",
    "image_encoder = Resnext50()\n",
    "# Switch model to the training mode\n",
    "image_encoder.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eac0ec",
   "metadata": {},
   "source": [
    "### Contrastive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb28745c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_encoder - ResNet or Vision Transformer\n",
    "# text_encoder - CBOW or Text Transformer\n",
    "# I[n, h, w, c] - minibatch of aligned images\n",
    "# T[n, l] - minibatch of aligned texts\n",
    "# W_i[d_i, d_e] - learned proj of image to embed\n",
    "# W_t[d_t, d_e] - learned proj of text to embed\n",
    "# t - learned temperature parameter\n",
    "# extract feature representations of each modality\n",
    "I_f = image_encoder(I) #[n, d_i]\n",
    "T_f = text_encoder(T) #[n, d_t]\n",
    "# joint multimodal embedding [n, d_e]\n",
    "I_e = l2_normalize(np.dot(I_f, W_i), axis=1)\n",
    "T_e = l2_normalize(np.dot(T_f, W_t), axis=1)\n",
    "# scaled pairwise cosine similarities [n, n]\n",
    "logits = np.dot(I_e, T_e.T) * np.exp(t)\n",
    "# symmetric loss function\n",
    "labels = np.arange(n)\n",
    "loss_i = cross_entropy_loss(logits, labels, axis=0)\n",
    "loss_t = cross_entropy_loss(logits, labels, axis=1)\n",
    "loss = (loss_i + loss_t)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6ef059",
   "metadata": {},
   "source": [
    "### GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41c3b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCondGenerator(nn.Module):\n",
    "    def __init__(self, z_dim=128, t_dim=256, img_size=256):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(z_dim + t_dim, 512 * 4 * 4)\n",
    "        # then deconv / upsampling blocks to get [1,H,W]\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 1, 4, 2, 1),\n",
    "            nn.Tanh()     # outputs in [-1,1]\n",
    "        )\n",
    "\n",
    "    def forward(self, z_noise, z_text):\n",
    "        z = torch.cat([z_noise, z_text], dim=-1)\n",
    "        x = self.fc(z)\n",
    "        x = x.view(x.size(0), 512, 4, 4)\n",
    "        img = self.main(x)\n",
    "        return img\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
